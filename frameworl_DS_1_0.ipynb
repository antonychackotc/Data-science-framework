{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdn1KZSNPP0kfBGQaHLe4x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonychackotc/Data-science-framework/blob/main/frameworl_DS_1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n",
        "!pip install -q pyngrok\n",
        "!pip install -q localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sege23Kz4Pw",
        "outputId": "bfecd12c-0fba-4f42-8d34-3b568b30180b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement localtunnel (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for localtunnel\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u1ERoN8z6n6",
        "outputId": "9aa1c0a8-9b80-4908-deb5-812e14bde9ee"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.11/dist-packages (1.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app1.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from fpdf import FPDF\n",
        "import os\n",
        "import io\n",
        "import base64\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, confusion_matrix)\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from io import StringIO\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, LeaveOneOut, LeavePOut, train_test_split\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Streamlit App Title\n",
        "st.title(\"Automated EDA and Machine Learning Framework\")\n",
        "\n",
        "# Top Navigation (Tabs)\n",
        "tabs = st.tabs([\"About\", \"Upload Dataset\", \"Dataset Analysis\", \"EDA Process\", \"Preprocess\", \"Machine Learning\"])\n",
        "\n",
        "\n",
        "with tabs[0]:\n",
        "    st.title(\"**Hyper Supra 1.O**\")\n",
        "    st.subheader(\"**Supra without brakes**\")\n",
        "\n",
        "    st.success(\"### Purpose:\")\n",
        "    st.write(\"- Automated Exploratory Data Analysis (EDA) with business insights.\")\n",
        "    st.write(\"- Machine learning capabilities for classification, regression, and clustering.\")\n",
        "\n",
        "    st.success(\"### Features:\")\n",
        "    features = [\n",
        "        \"Automatic Summary Statistics: Provides quick insights into dataset structure.\",\n",
        "        \"Visualization Tools: Includes histograms, scatterplots, and boxplots for data exploration.\",\n",
        "        \"PCA for Dimensionality Reduction: Reduces feature space for better analysis.\",\n",
        "        \"Outlier Detection: Identifies anomalies in the dataset.\",\n",
        "        \"Correlation Insights: Highlights relationships between variables.\",\n",
        "        \"Supervised Learning: Supports classification and regression tasks.\",\n",
        "        \"Unsupervised Learning: Includes clustering algorithms like KMeans, DBSCAN, and Agglomerative Clustering.\",\n",
        "        \"Cross-Validation and Model Evaluation: Ensures robust model performance.\",\n",
        "        \"Feature Engineering and Preprocessing: Handles missing values, encoding, and scaling.\",\n",
        "        \"Business Insights: Provides actionable conclusions for decision-making.\",\n",
        "        \"Downloadable Reports and Datasets: Allows users to export results and cleaned datasets.\"\n",
        "    ]\n",
        "    for feature in features:\n",
        "        st.write(f\"- {feature}\")\n",
        "\n",
        "    st.success(\"### Instructions:\")\n",
        "    instructions = [\n",
        "        \"Upload Dataset: Upload datasets in CSV, Excel, or JSON format.\",\n",
        "        \"Dataset Analysis: Explore summary statistics, outliers, and missing values.\",\n",
        "        \"EDA Process: Perform univariate, bivariate, and multivariate analysis.\",\n",
        "        \"Preprocess: Handle missing values, encode categorical variables, and scale features.\",\n",
        "        \"Machine Learning: Train and evaluate supervised and unsupervised models.\"\n",
        "    ]\n",
        "    for instruction in instructions:\n",
        "        st.write(f\"- {instruction}\")\n",
        "\n",
        "    st.success(\"### Dependencies:\")\n",
        "    dependencies = \"Streamlit, Pandas, NumPy, Scikit-learn, Seaborn, Matplotlib, XGBoost, LightGBM, Pandas Profiling.\"\n",
        "    st.write(dependencies)\n",
        "\n",
        "    st.write(\"### Developer:\")\n",
        "    st.write(\"TC.Antony - Data Scientist.\")\n",
        "\n",
        "    st.info(\"This tab serves as the introduction to the Hyper Supra 1.O framework, outlining its purpose, features, and how users can navigate through the tool to achieve their data analysis and machine learning goals.\")\n",
        "\n",
        "\n",
        "# Upload Dataset Section\n",
        "with tabs[1]:\n",
        "    st.header(\"Upload Your Dataset\")\n",
        "\n",
        "    uploaded_files = st.file_uploader(\"Upload CSV, Excel, JSON, or Pickle files\", type=[\"csv\", \"xlsx\", \"json\", \"pkl\"], accept_multiple_files=True)\n",
        "\n",
        "    if uploaded_files:\n",
        "        data_frames = []\n",
        "        file_names = []\n",
        "\n",
        "        for uploaded_file in uploaded_files:\n",
        "            file_names.append(uploaded_file.name)\n",
        "            if uploaded_file.type == \"application/vnd.ms-excel\" or uploaded_file.type == \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\":\n",
        "                df = pd.read_excel(uploaded_file)\n",
        "            elif uploaded_file.type == \"application/json\":\n",
        "                df = pd.read_json(uploaded_file)\n",
        "            elif uploaded_file.type == \"application/octet-stream\":\n",
        "                df = pd.read_pickle(uploaded_file)\n",
        "            else:\n",
        "                df = pd.read_csv(uploaded_file)\n",
        "            data_frames.append(df)\n",
        "\n",
        "        st.sidebar.header(\"Select Dataset\")\n",
        "        selected_dataset_name = st.sidebar.selectbox(\"Choose a dataset\", file_names)\n",
        "        selected_index = file_names.index(selected_dataset_name)\n",
        "        df = data_frames[selected_index]\n",
        "\n",
        "        st.write(\"### Data Preview\")\n",
        "        st.write(df.head())\n",
        "        st.write(\"### Data Summary\")\n",
        "        st.write(df.describe())\n",
        "\n",
        "        # Save the dataframe for further analysis\n",
        "        st.session_state.df = df\n",
        "\n",
        "# Dataset Analysis Section\n",
        "with tabs[2]:\n",
        "    if 'df' in st.session_state:\n",
        "        st.header(f\"Dataset Analysis for {selected_dataset_name}\")\n",
        "        df = st.session_state.df\n",
        "\n",
        "        # Create subtabs for Dataset Analysis\n",
        "        data_analysis_tabs = st.tabs([\"Dataset Preview\", \"Outliers\", \"Standard Deviation\", \"Multicollinearity\", \"Missing Values\", \"Preprocessing Preview\"])\n",
        "\n",
        "        # Dataset Preview Tab\n",
        "        with data_analysis_tabs[0]:\n",
        "            st.subheader(\"Dataset Preview\")\n",
        "            total_rows, total_columns = df.shape\n",
        "            st.write(f\"**Total Rows:** {total_rows}\")\n",
        "            st.write(f\"**Total Columns:** {total_columns}\")\n",
        "            st.write(\"**Shape of the Dataset:**\")\n",
        "            st.write(df.shape)\n",
        "            missing_values = df.isnull().sum()\n",
        "            missing_values = missing_values[missing_values > 0]\n",
        "            st.write(\"**Missing Values (isnull):**\")\n",
        "            st.write(missing_values)\n",
        "            categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "            numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "            st.write(\"**Categorical Columns:**\")\n",
        "            st.write(categorical_cols)\n",
        "            st.write(\"**Numerical Columns:**\")\n",
        "            st.write(numerical_cols)\n",
        "\n",
        "        # Outliers Detection Tab\n",
        "        with data_analysis_tabs[1]:\n",
        "            st.subheader(\"Outlier Detection (based on numeric columns):\")\n",
        "            numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "            for col in numeric_cols:\n",
        "                q1 = df[col].quantile(0.25)\n",
        "                q3 = df[col].quantile(0.75)\n",
        "                iqr = q3 - q1\n",
        "                lower_boundary = q1 - 1.5 * iqr\n",
        "                upper_boundary = q3 + 1.5 * iqr\n",
        "                lower_outliers = df[df[col] < lower_boundary]\n",
        "                upper_outliers = df[df[col] > upper_boundary]\n",
        "                st.write(f\"**{col}**:\")\n",
        "                st.write(f\"  - Total number of lower boundary-based outliers: {len(lower_outliers)}\")\n",
        "                st.write(f\"  - Total number of upper boundary-based outliers: {len(upper_outliers)}\")\n",
        "\n",
        "        # Standard Deviation Tab\n",
        "        with data_analysis_tabs[2]:\n",
        "            st.subheader(\"Standard Deviation Analysis:\")\n",
        "            std_devs = df[numeric_cols].std().to_dict()\n",
        "            st.write(\"Standard deviation of each numeric column:\")\n",
        "            st.write(std_devs)\n",
        "\n",
        "        # Multicollinearity Tab\n",
        "        with data_analysis_tabs[3]:\n",
        "            st.subheader(\"Identical Standard Deviations and Multicollinearity (Redundant Feature Detection)\")\n",
        "            std_devs = df[numeric_cols].std().round(6)\n",
        "            std_dev_groups = std_devs.groupby(std_devs).groups\n",
        "            redundant_features = []\n",
        "            for std_value, cols in std_dev_groups.items():\n",
        "                if len(cols) > 1:\n",
        "                    redundant_features.append(list(cols))\n",
        "            if redundant_features:\n",
        "                st.write(\"**Columns with identical standard deviations (potentially redundant):**\")\n",
        "                for group in redundant_features:\n",
        "                    st.write(group)\n",
        "                    st.write(\"Consider selecting one of these columns to reduce multicollinearity.\")\n",
        "                flat_redundant_features = [col for group in redundant_features for col in group]\n",
        "                cols_to_delete = st.multiselect(\"Select columns to delete:\", options=flat_redundant_features)\n",
        "                if st.button(\"Delete Selected Columns\"):\n",
        "                    if cols_to_delete:\n",
        "                        df = df.drop(columns=cols_to_delete)\n",
        "                        st.success(f\"Deleted columns: {', '.join(cols_to_delete)}\")\n",
        "                        st.write(\"Updated DataFrame:\")\n",
        "                        st.write(df)\n",
        "                        csv_data = df.to_csv(index=False)\n",
        "                        st.download_button(\"Download Updated Dataset\", data=csv_data, file_name=\"updated_dataset.csv\", mime=\"text/csv\")\n",
        "                    else:\n",
        "                        st.warning(\"No columns selected for deletion.\")\n",
        "            else:\n",
        "                st.write(\"No columns found with identical standard deviations.\")\n",
        "            st.subheader(\"What is Multicollinearity?\")\n",
        "            st.write(\"\"\"\n",
        "              Multicollinearity occurs when two or more independent variables are highly correlated.\n",
        "              This can lead to unstable coefficient estimates in models, reduced interpretability, and inefficient models.\n",
        "\n",
        "              **Why Reduce Multicollinearity?**\n",
        "              - Improves model efficiency by reducing redundant information.\n",
        "              - Enhances the stability and reliability of regression coefficients.\n",
        "              - Boosts model interpretability.\n",
        "\n",
        "              **Example:**\n",
        "              If columns `Monthly_Sales` and `Quarterly_Sales` (which is 3 times `Monthly_Sales`) are present, removing one will reduce multicollinearity.\n",
        "              Identical standard deviations may also indicate redundant features carrying overlapping information.\n",
        "            \"\"\")\n",
        "\n",
        "        # Missing Value Handling Subtab\n",
        "        with data_analysis_tabs[4]:\n",
        "            st.header(\"Handling Missing Values\")\n",
        "            missing_values = df.isnull().sum()\n",
        "            st.write(\"Missing Values per Column:\")\n",
        "            st.write(missing_values[missing_values > 0])\n",
        "\n",
        "            def handle_missing_value(col, missing_values):\n",
        "                missing_count = missing_values[col]\n",
        "                st.header(f\"Column: {col}\")\n",
        "                st.write(f\"Missing Values: {missing_count}\")\n",
        "                if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                    q1 = df[col].quantile(0.25)\n",
        "                    q3 = df[col].quantile(0.75)\n",
        "                    iqr = q3 - q1\n",
        "                    outliers = df[(df[col] < (q1 - 1.5 * iqr)) | (df[col] > (q3 + 1.5 * iqr))]\n",
        "                    outlier_count = len(outliers)\n",
        "                    st.write(f\"Outliers in {col}: {outlier_count}\")\n",
        "                    if outlier_count > 0:\n",
        "                        st.write(f\"**Outliers Detected** in {col}.\")\n",
        "                    else:\n",
        "                        st.write(f\"No **outliers** detected in {col}.\")\n",
        "                    skewness_value = df[col].dropna().skew()\n",
        "                    is_normal = \"Yes\" if -1 < skewness_value < 1 else \"No\"\n",
        "                    st.write(f\"Skewness: {skewness_value:.2f}\")\n",
        "                    st.write(f\"Is {col} approximately normally distributed? {is_normal}\")\n",
        "                    sns.histplot(df[col].dropna(), kde=True)\n",
        "                    st.pyplot()\n",
        "                else:\n",
        "                    st.write(f\"{col} is not numeric. Skipping outlier and normality analysis.\")\n",
        "                if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                    if skewness_value > 1:\n",
        "                        recommended_fill_option = \"Median\"\n",
        "                    else:\n",
        "                        recommended_fill_option = \"Mean\"\n",
        "                else:\n",
        "                    recommended_fill_option = \"Mode\"\n",
        "                st.info(f\"**Recommended Filling for {col}: {recommended_fill_option}**\")\n",
        "                fill_option = st.selectbox(\n",
        "                    f\"Choose a filling option for {col}:\",\n",
        "                    options=[\"Select\", \"Mean\", \"Median\", \"Mode\", \"Forward Fill\", \"Backward Fill\", \"Custom\"],\n",
        "                    index=[\"Mean\", \"Median\", \"Mode\"].index(recommended_fill_option) + 1,\n",
        "                    key=f\"fill_option_{col}\"\n",
        "                )\n",
        "                if fill_option == \"Custom\":\n",
        "                    custom_value = st.text_input(f\"Enter custom fill value for {col}:\")\n",
        "                    if custom_value:\n",
        "                        df[col] = df[col].fillna(custom_value)\n",
        "                        st.write(f\"Filled missing values in {col} with custom value: {custom_value}\")\n",
        "                elif fill_option == \"Mean\":\n",
        "                    if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                        df[col] = df[col].fillna(df[col].mean())\n",
        "                        st.write(f\"Filled missing values in {col} with the mean.\")\n",
        "                    else:\n",
        "                        st.write(f\"{col} is not numeric, so 'Mean' filling is not applicable.\")\n",
        "                elif fill_option == \"Median\":\n",
        "                    if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                        df[col] = df[col].fillna(df[col].median())\n",
        "                        st.write(f\"Filled missing values in {col} with the median.\")\n",
        "                    else:\n",
        "                        st.write(f\"{col} is not numeric, so 'Median' filling is not applicable.\")\n",
        "                elif fill_option == \"Mode\":\n",
        "                    df[col] = df[col].fillna(df[col].mode()[0])\n",
        "                    st.write(f\"Filled missing values in {col} with the mode.\")\n",
        "                elif fill_option == \"Forward Fill\":\n",
        "                    df[col] = df[col].fillna(method='ffill')\n",
        "                    st.write(f\"Filled missing values in {col} using forward fill.\")\n",
        "                elif fill_option == \"Backward Fill\":\n",
        "                    df[col] = df[col].fillna(method='bfill')\n",
        "                    st.write(f\"Filled missing values in {col} using backward fill.\")\n",
        "                missing_values = df.isnull().sum()\n",
        "                st.write(\"Missing Values per Column After Filling:\")\n",
        "                st.write(missing_values[missing_values > 0])\n",
        "                st.write(f\"Missing values in {col} have been handled.\")\n",
        "                return missing_values\n",
        "\n",
        "            if st.button(\"Fill Missing Values for All Columns at Once\"):\n",
        "                for col in df.columns:\n",
        "                    if missing_values[col] > 0:\n",
        "                        missing_values = handle_missing_value(col, missing_values)\n",
        "                st.write(\"All missing values have been filled.\")\n",
        "                missing_values = df.isnull().sum()\n",
        "                st.write(\"Missing Values per Column After All Fills:\")\n",
        "                st.write(missing_values[missing_values > 0])\n",
        "\n",
        "            def download_file(df):\n",
        "                csv = df.to_csv(index=False)\n",
        "                b64 = base64.b64encode(csv.encode()).decode()\n",
        "                href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"cleaned_data.csv\">Download Cleaned Dataset</a>'\n",
        "                return href\n",
        "\n",
        "            st.markdown(download_file(df), unsafe_allow_html=True)\n",
        "\n",
        "        # Preprocessing Preview Tab\n",
        "        with data_analysis_tabs[5]:\n",
        "            preprocessing_tabs = st.tabs([\"Encoding Recommendations\", \"Feature Scaling Recommendations\"])\n",
        "\n",
        "            # Encoding Recommendations Tab\n",
        "            with preprocessing_tabs[0]:\n",
        "                st.subheader(\"Encoding Recommendations\")\n",
        "                categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "                ordinal_cols = [col for col in categorical_cols if df[col].nunique() <= 5]\n",
        "                nominal_cols = [col for col in categorical_cols if df[col].nunique() > 5]\n",
        "                if ordinal_cols:\n",
        "                    st.write(\"**Columns recommended for Ordinal Encoding:**\")\n",
        "                    st.write(ordinal_cols)\n",
        "                else:\n",
        "                    st.write(\"No columns recommended for ordinal encoding.\")\n",
        "                if nominal_cols:\n",
        "                    st.write(\"**Columns recommended for Nominal Encoding:**\")\n",
        "                    st.write(nominal_cols)\n",
        "                else:\n",
        "                    st.write(\"No columns recommended for nominal encoding.\")\n",
        "\n",
        "            # Feature Scaling Recommendations Tab\n",
        "            with preprocessing_tabs[1]:\n",
        "                st.subheader(\"Feature Scaling Recommendations\")\n",
        "                numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "                normalization_cols = [col for col in numeric_cols if df[col].min() >= 0 and df[col].max() - df[col].min() > 1000]\n",
        "                standardization_cols = [col for col in numeric_cols if df[col].min() < 0 or (df[col].std() > 1 and df[col].max() - df[col].min() <= 1000)]\n",
        "                if normalization_cols:\n",
        "                    st.write(\"**Columns requiring normalization (Min-Max Scaling):**\")\n",
        "                    st.write(normalization_cols)\n",
        "                else:\n",
        "                    st.write(\"No columns requiring normalization.\")\n",
        "                if standardization_cols:\n",
        "                    st.write(\"**Columns requiring standardization (Z-Score):**\")\n",
        "                    st.write(standardization_cols)\n",
        "                else:\n",
        "                    st.write(\"No columns requiring standardization.\")\n",
        "                st.subheader(\"Why Normalization and Standardization?\")\n",
        "                st.write(\"\"\"\n",
        "                    **Normalization (Min-Max Scaling):** This technique scales the data between 0 and 1. It is useful when the data values are positive and vary widely. It preserves the relationships in the data without changing its distribution. Susceptile to outliers - Bound to Range eg: 0 to 1 - Data Shape Does't Change\n",
        "                    **Standardization (Z-Score):** This technique scales the data to have a mean of 0 and a standard deviation of 1. It is useful when data values have both positive and negative values or follow a normal distribution. Less Prone to Outliers(best for Outliers) -  No Range Constraint - Data Shape Changes\n",
        "                \"\"\")\n",
        "\n",
        "# EDA Process Section\n",
        "with tabs[3]:\n",
        "    if 'df' not in st.session_state:\n",
        "        st.warning(\"Please upload a dataset first!\")\n",
        "    else:\n",
        "        df = st.session_state.df\n",
        "        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
        "        st.info(f\"#### Numeric Columns: {', '.join(numeric_columns)}\" if numeric_columns else \"No numeric columns found\")\n",
        "        st.success(f\"#### Categorical Columns: {', '.join(categorical_columns)}\" if categorical_columns else \"No categorical columns found\")\n",
        "        pdf = FPDF()\n",
        "        pdf.set_auto_page_break(auto=True, margin=15)\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        pdf.cell(200, 10, txt=\"Exploratory Data Analysis Report\", ln=True, align='C')\n",
        "        pdf.ln(10)\n",
        "        output_dir = '/mnt/data/eda_images/'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        eda_tabs = st.tabs([\"Univariate Analysis\", \"Bivariate Analysis\", \"Multivariate Analysis\", \"Business Insights\", \"Download Report\"])\n",
        "\n",
        "        # Univariate Analysis Tab\n",
        "        with eda_tabs[0]:\n",
        "            st.write(\"### 1) Univariate Analysis\")\n",
        "            selected_univariate_column = st.selectbox(\"Select Column for Univariate Analysis\", df.columns, index=0)\n",
        "            st.write(f\"#### Summary Statistics for '{selected_univariate_column}'\")\n",
        "            st.write(df[selected_univariate_column].describe())\n",
        "            st.write(f\"#### Histogram for '{selected_univariate_column}'\")\n",
        "            fig, ax = plt.subplots()\n",
        "            sns.histplot(df[selected_univariate_column], kde=True, ax=ax, color=\"blue\")\n",
        "            st.pyplot(fig)\n",
        "            fig.savefig(f'{output_dir}histogram_{selected_univariate_column}.png')\n",
        "            pdf.cell(200, 10, txt=f\"Histogram of '{selected_univariate_column}':\", ln=True)\n",
        "            pdf.image(f'{output_dir}histogram_{selected_univariate_column}.png', x=10, y=None, w=100)\n",
        "            st.write(f\"#### Boxplot for '{selected_univariate_column}'\")\n",
        "            fig, ax = plt.subplots()\n",
        "            sns.boxplot(x=df[selected_univariate_column], ax=ax, color=\"green\")\n",
        "            st.pyplot(fig)\n",
        "            fig.savefig(f'{output_dir}boxplot_{selected_univariate_column}.png')\n",
        "            pdf.cell(200, 10, txt=f\"Boxplot of '{selected_univariate_column}':\", ln=True)\n",
        "            pdf.image(f'{output_dir}boxplot_{selected_univariate_column}.png', x=10, y=None, w=100)\n",
        "            if df[selected_univariate_column].dtype == 'object':\n",
        "                st.write(f\"#### Count Plot for '{selected_univariate_column}'\")\n",
        "                fig, ax = plt.subplots()\n",
        "                sns.countplot(x=df[selected_univariate_column], ax=ax, palette=\"Set2\")\n",
        "                st.pyplot(fig)\n",
        "                fig.savefig(f'{output_dir}countplot_{selected_univariate_column}.png')\n",
        "                pdf.cell(200, 10, txt=f\"Count Plot of '{selected_univariate_column}':\", ln=True)\n",
        "                pdf.image(f'{output_dir}countplot_{selected_univariate_column}.png', x=10, y=None, w=100)\n",
        "                pie_data = df[selected_univariate_column].value_counts()\n",
        "                if pie_data.shape[0] > 1:\n",
        "                    st.write(f\"#### Pie Chart for '{selected_univariate_column}'\")\n",
        "                    fig, ax = plt.subplots()\n",
        "                    pie_data.plot.pie(autopct='%1.1f%%', startangle=90, ax=ax, colors=sns.color_palette(\"Set2\", len(pie_data)))\n",
        "                    ax.set_ylabel('')\n",
        "                    st.pyplot(fig)\n",
        "                    fig.savefig(f'{output_dir}pieplot_{selected_univariate_column}.png')\n",
        "                    pdf.cell(200, 10, txt=f\"Pie Chart of '{selected_univariate_column}':\", ln=True)\n",
        "                    pdf.image(f'{output_dir}pieplot_{selected_univariate_column}.png', x=10, y=None, w=100)\n",
        "                else:\n",
        "                    st.write(f\"Pie chart cannot be created for '{selected_univariate_column}' because it contains only one category.\")\n",
        "\n",
        "        # Bivariate Analysis Tab\n",
        "        with eda_tabs[1]:\n",
        "            st.write(\"### 2) Bivariate Analysis\")\n",
        "            selected_bivariate_columns = st.multiselect(\n",
        "                \"Select 2 Columns for Bivariate Analysis\",\n",
        "                numeric_columns + categorical_columns,\n",
        "                default=[numeric_columns[0], numeric_columns[1]]\n",
        "            )\n",
        "            if len(selected_bivariate_columns) == 2:\n",
        "                x_column, y_column = selected_bivariate_columns\n",
        "                st.write(f\"#### Scatter Plot for '{x_column}' vs '{y_column}'\")\n",
        "                fig, ax = plt.subplots()\n",
        "                sns.scatterplot(data=df, x=x_column, y=y_column, ax=ax, color='purple')\n",
        "                st.pyplot(fig)\n",
        "                fig.savefig(f'{output_dir}scatterplot_{x_column}_{y_column}.png')\n",
        "                pdf.cell(200, 10, txt=f\"Scatter Plot between '{x_column}' and '{y_column}':\", ln=True)\n",
        "                pdf.image(f'{output_dir}scatterplot_{x_column}_{y_column}.png', x=10, y=None, w=100)\n",
        "                st.write(f\"#### Count Plot for Crosstab of '{x_column}' vs '{y_column}'\")\n",
        "                crosstab = pd.crosstab(df[x_column], df[y_column])\n",
        "                st.write(crosstab)\n",
        "                desired_bins = st.number_input(\"Enter Desired Number of Groups (e.g., 10, 20)\", min_value=0, max_value=100, value=0, key=\"unique_desired_bins_input\")\n",
        "                if desired_bins > 0:\n",
        "                    min_val = df[x_column].min()\n",
        "                    max_val = df[x_column].max()\n",
        "                    if min_val == max_val:\n",
        "                        st.warning(\"All values in the selected column are the same. Cannot create bins.\")\n",
        "                    else:\n",
        "                        bin_edges = np.linspace(min_val, max_val, desired_bins + 1)\n",
        "                        df['group'] = pd.cut(df[x_column], bins=bin_edges, include_lowest=True)\n",
        "                        df['group'] = df['group'].apply(lambda x: f\"({int(x.left)},{int(x.right)})\" if pd.notnull(x) else \"NaN\")\n",
        "                        grouped_data = df.groupby([x_column, 'group', y_column]).size().reset_index(name='Count')\n",
        "                        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "                        sns.barplot(x='group', y='Count', hue=y_column, data=grouped_data, palette='Set2', ax=ax)\n",
        "                        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "                        st.pyplot(fig)\n",
        "                        fig.savefig(f'{output_dir}countplot_grouped_{x_column}_{y_column}_binned.png')\n",
        "                        pdf.cell(200, 10, txt=f\"Grouped Count Plot for '{x_column}' and '{y_column}' (Binned):\", ln=True)\n",
        "                        pdf.image(f'{output_dir}countplot_grouped_{x_column}_{y_column}_binned.png', x=10, y=None, w=100)\n",
        "                        crosstab_binned = pd.crosstab(df['group'], df[y_column])\n",
        "                        crosstab_percent_binned = crosstab_binned.apply(lambda x: x / x.sum() * 100, axis=1)\n",
        "                        crosstab_percent_binned = crosstab_percent_binned.reset_index()\n",
        "                        merged_df = pd.merge(df[['group']], crosstab_percent_binned, on='group', how='left')\n",
        "                        unique_group_percent = merged_df.groupby('group').sum().reset_index()\n",
        "                        unique_group_percent.iloc[:, 1:] = unique_group_percent.iloc[:, 1:].div(unique_group_percent.iloc[:, 1:].sum(axis=1), axis=0) * 100\n",
        "                        st.write(f\"#### Unique Groups in '{x_column}' with Percentage Distribution Summing to 100%\")\n",
        "                        st.dataframe(unique_group_percent)\n",
        "                        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                        crosstab_percent_binned_numeric = crosstab_percent_binned.drop(columns=['group'])\n",
        "                        sns.heatmap(crosstab_percent_binned_numeric, annot=True, cmap='YlGnBu', ax=ax, fmt='.2f')\n",
        "                        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "                        st.pyplot(fig)\n",
        "                        fig.savefig(f'{output_dir}crosstab_percent_heatmap_binned_{x_column}_{y_column}.png')\n",
        "                        pdf.cell(200, 10, txt=f\"Crosstab Percentage Heatmap for '{x_column}' and '{y_column}' (Binned):\", ln=True)\n",
        "                        pdf.image(f'{output_dir}crosstab_percent_heatmap_binned_{x_column}_{y_column}.png', x=10, y=None, w=100)\n",
        "                else:\n",
        "                    grouped_data = df.groupby([x_column, y_column]).size().reset_index(name='Count')\n",
        "                    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "                    sns.barplot(x=x_column, y='Count', hue=y_column, data=grouped_data, palette='Set2', ax=ax)\n",
        "                    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "                    st.pyplot(fig)\n",
        "                    fig.savefig(f'{output_dir}countplot_grouped_{x_column}_{y_column}.png')\n",
        "                    pdf.cell(200, 10, txt=f\"Grouped Count Plot for '{x_column}' and '{y_column}':\", ln=True)\n",
        "                    pdf.image(f'{output_dir}countplot_grouped_{x_column}_{y_column}.png', x=10, y=None, w=100)\n",
        "                if desired_bins == 0:\n",
        "                    st.write(\"### Ungrouped Data Visualization\")\n",
        "                    st.dataframe(df[[x_column, y_column]])\n",
        "                    st.write(\"### Raw Data Plot\")\n",
        "                    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "                    sns.histplot(df[x_column], kde=True, ax=ax, color='skyblue', bins=30)\n",
        "                    ax.set_title(f\"Histogram of {x_column}\")\n",
        "                    st.pyplot(fig)\n",
        "                else:\n",
        "                    min_val = df[x_column].min()\n",
        "                    max_val = df[x_column].max()\n",
        "                    if min_val == max_val:\n",
        "                        st.warning(\"All values in the selected column are the same. Cannot create bins.\")\n",
        "                    else:\n",
        "                        bin_edges = np.linspace(min_val, max_val, desired_bins + 1)\n",
        "                        df['group'] = pd.cut(df[x_column], bins=bin_edges, include_lowest=True)\n",
        "                        df['group'] = df['group'].apply(lambda x: f\"({int(x.left)},{int(x.right)})\" if pd.notnull(x) else \"NaN\")\n",
        "                        grouped_summary = df.groupby(['group']).size().reset_index(name='Count')\n",
        "                        st.write(\"### Grouped Data Summary\")\n",
        "                        st.dataframe(grouped_summary)\n",
        "\n",
        "        # Multivariate Analysis Tab\n",
        "        with eda_tabs[2]:\n",
        "            st.write(\"### 3) Multivariate Analysis\")\n",
        "            st.write(\"#### Correlation Heatmap\")\n",
        "            selected_corr_columns = st.multiselect(\"Select Columns for Correlation Heatmap\", numeric_columns, default=numeric_columns)\n",
        "            if len(selected_corr_columns) > 1:\n",
        "                correlation_matrix = df[selected_corr_columns].corr()\n",
        "                fig, ax = plt.subplots()\n",
        "                sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ax=ax)\n",
        "                st.pyplot(fig)\n",
        "                fig.savefig(f'{output_dir}correlation_heatmap.png')\n",
        "                pdf.cell(200, 10, txt=\"Correlation Heatmap:\", ln=True)\n",
        "                pdf.image(f'{output_dir}correlation_heatmap.png', x=10, y=None, w=100)\n",
        "                st.write(\"#### PCA Analysis\")\n",
        "                selected_pca_columns = st.multiselect(\"Select Columns for PCA Analysis\", numeric_columns, default=numeric_columns)\n",
        "                if len(selected_pca_columns) > 1:\n",
        "                    df_numeric = df[selected_pca_columns].fillna(0)\n",
        "                    pca = PCA(n_components=2)\n",
        "                    pca_result = pca.fit_transform(df_numeric)\n",
        "                    st.write(\"#### PCA 2D Plot\")\n",
        "                    pca_df = pd.DataFrame(pca_result, columns=[\"PC1\", \"PC2\"])\n",
        "                    st.write(f\"#### PC2 is based on: {', '.join(selected_pca_columns)}\")\n",
        "                    fig, ax = plt.subplots()\n",
        "                    sns.scatterplot(data=pca_df, x=\"PC1\", y=\"PC2\", ax=ax, color='orange')\n",
        "                    st.pyplot(fig)\n",
        "                    fig.savefig(f'{output_dir}pca_plot.png')\n",
        "                    pdf.cell(200, 10, txt=\"PCA 2D Plot:\", ln=True)\n",
        "                    pdf.image(f'{output_dir}pca_plot.png', x=10, y=None, w=100)\n",
        "\n",
        "        # Business Insights Tab\n",
        "        with eda_tabs[3]:\n",
        "            st.write(\"### Business Insights\")\n",
        "            insights = []\n",
        "            numeric_cols = df.select_dtypes(include=np.number)\n",
        "            if not numeric_cols.empty:\n",
        "                fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                sns.heatmap(numeric_cols.corr(), annot=True, fmt=\".2f\", cmap='Blues', ax=ax)\n",
        "                st.pyplot(fig)\n",
        "                correlation_matrix = numeric_cols.corr()\n",
        "                high_corr_pairs = [(col1, col2) for col1 in correlation_matrix.columns for col2 in correlation_matrix.columns if col1 != col2 and abs(correlation_matrix.loc[col1, col2]) > 0.8]\n",
        "                if high_corr_pairs:\n",
        "                    insights.append(\"Strong correlations detected between pairs of columns:\")\n",
        "                    for col1, col2 in high_corr_pairs:\n",
        "                        insights.append(f\"- '{col1}' and '{col2}' show high correlation ({correlation_matrix.loc[col1, col2]:.2f})\")\n",
        "            if numeric_cols.shape[1] > 1:\n",
        "                st.write(\"#### PCA Analysis (Top 2 Components)\")\n",
        "                pca = PCA(n_components=2)\n",
        "                pca_result = pca.fit_transform(numeric_cols.dropna())\n",
        "                explained_variance = pca.explained_variance_ratio_\n",
        "                insights.append(f\"PCA Component 1 explains {explained_variance[0]:.2%} of the variance.\")\n",
        "                insights.append(f\"PCA Component 2 explains {explained_variance[1]:.2%} of the variance.\")\n",
        "                pca_df = pd.DataFrame(pca_result, columns=[\"PC1\", \"PC2\"])\n",
        "                fig, ax = plt.subplots(figsize=(8, 5))\n",
        "                sns.scatterplot(data=pca_df, x=\"PC1\", y=\"PC2\")\n",
        "                st.pyplot(fig)\n",
        "            st.write(\"#### Custom Conclusion:\")\n",
        "            custom_conclusion = st.text_area(\"Write your conclusion here:\", height=150)\n",
        "            if custom_conclusion:\n",
        "                st.write(\"**Your Custom Conclusion:**\")\n",
        "                st.write(custom_conclusion)\n",
        "            else:\n",
        "                st.write(\"You can add a custom conclusion by typing in the box above.\")\n",
        "            if insights:\n",
        "                st.write(\"#### Business Insights:\")\n",
        "                for insight in insights:\n",
        "                    st.write(f\"- {insight}\")\n",
        "            else:\n",
        "                st.write(\"No significant insights found.\")\n",
        "\n",
        "        # Download Report Tab\n",
        "        with eda_tabs[4]:\n",
        "            st.write(\"### 5) Download the Full Report\")\n",
        "            pdf = FPDF()\n",
        "            pdf.set_auto_page_break(auto=True, margin=15)\n",
        "            pdf.add_page()\n",
        "            pdf.set_font(\"Arial\", \"B\", 16)\n",
        "            pdf.cell(200, 10, \"Exploratory Data Analysis Report\", ln=True, align=\"C\")\n",
        "            pdf.ln(10)\n",
        "            pdf.set_font(\"Arial\", size=12)\n",
        "            pdf.cell(200, 10, txt=\"This is an EDA report that summarizes key insights from the dataset.\", ln=True)\n",
        "            pdf.ln(10)\n",
        "            pdf.cell(200, 10, txt=\"### 1) Univariate Analysis\", ln=True)\n",
        "            pdf.ln(5)\n",
        "            for column in numeric_columns + categorical_columns:\n",
        "                try:\n",
        "                    pdf.cell(200, 10, txt=f\"Univariate Analysis for '{column}':\", ln=True)\n",
        "                    pdf.image(f'{output_dir}histogram_{column}.png', x=10, y=None, w=100)\n",
        "                    pdf.image(f'{output_dir}boxplot_{column}.png', x=10, y=None, w=100)\n",
        "                    if df[column].dtype == 'object':\n",
        "                        pdf.image(f'{output_dir}countplot_{column}.png', x=10, y=None, w=100)\n",
        "                    pdf.ln(10)\n",
        "                except:\n",
        "                    continue\n",
        "            pdf.cell(200, 10, txt=\"### 2) Bivariate Analysis\", ln=True)\n",
        "            pdf.ln(5)\n",
        "            for x_column, y_column in zip(numeric_columns, numeric_columns[1:]):\n",
        "                try:\n",
        "                    pdf.cell(200, 10, txt=f\"Bivariate Analysis for '{x_column}' vs '{y_column}':\", ln=True)\n",
        "                    pdf.image(f'{output_dir}scatterplot_{x_column}_{y_column}.png', x=10, y=None, w=100)\n",
        "                    pdf.image(f'{output_dir}countplot_grouped_{x_column}_{y_column}.png', x=10, y=None, w=100)\n",
        "                    pdf.ln(10)\n",
        "                except:\n",
        "                    continue\n",
        "            pdf.cell(200, 10, txt=\"### 3) Multivariate Analysis\", ln=True)\n",
        "            pdf.ln(5)\n",
        "            try:\n",
        "                pdf.cell(200, 10, txt=\"Correlation Heatmap:\", ln=True)\n",
        "                pdf.image(f'{output_dir}correlation_heatmap.png', x=10, y=None, w=100)\n",
        "                pdf.ln(10)\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                pdf.cell(200, 10, txt=\"PCA Analysis 2D Plot:\", ln=True)\n",
        "                pdf.image(f'{output_dir}pca_plot.png', x=10, y=None, w=100)\n",
        "                pdf.ln(10)\n",
        "            except:\n",
        "                pass\n",
        "            # Business Insights\n",
        "            pdf.cell(200, 10, txt=\"### 4) Business Insights\", ln=True)\n",
        "            pdf.ln(5)\n",
        "            # Add insights from heatmaps, correlations, and PCA\n",
        "            for insight in insights:\n",
        "              pdf.multi_cell(0, 10, txt=f\"- {insight}\", align='L')\n",
        "            pdf.ln(10)\n",
        "\n",
        "            # Custom Conclusion Section in the PDF\n",
        "            pdf.cell(200, 10, txt=\"### 5) Custom Conclusion\", ln=True)\n",
        "            pdf.ln(5)\n",
        "            if custom_conclusion:\n",
        "              pdf.multi_cell(0, 10, txt=f\"**Your Custom Conclusion:**\\n{custom_conclusion}\", align='L')\n",
        "            else:\n",
        "                pdf.multi_cell(0, 10, txt=\"You can add a custom conclusion by typing in the box above.\", align='L')\n",
        "            pdf.ln(10)\n",
        "\n",
        "            # Save PDF to /mnt/data\n",
        "            pdf_output = '/mnt/data/EDA_Report.pdf'\n",
        "            pdf.output(pdf_output)\n",
        "\n",
        "            # Read PDF file as bytes for downloading\n",
        "            with open(pdf_output, \"rb\") as f:\n",
        "               pdf_bytes = f.read()\n",
        "\n",
        "            # Streamlit download button\n",
        "            st.download_button(\n",
        "                label=\"Download Full Report (PDF)\",\n",
        "                data=pdf_bytes,\n",
        "                file_name=\"EDA_Report.pdf\",\n",
        "                mime=\"application/pdf\"\n",
        "            )\n",
        "\n",
        "with tabs[4]:\n",
        "    st.write(\"### Preprocessing Tab\")\n",
        "\n",
        "    # Sub-tabs for Encoding and Feature Scaling\n",
        "    preprocessing_tabs = st.tabs([\"Encoding Recommendations\", \"Feature Scaling Recommendations\"])\n",
        "\n",
        "    # Encoding Recommendations Tab\n",
        "    with preprocessing_tabs[0]:\n",
        "        st.subheader(\"Encoding Recommendations\")\n",
        "\n",
        "        # Identify categorical columns\n",
        "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "        # Identify columns for ordinal and nominal encoding\n",
        "        ordinal_cols = [col for col in categorical_cols if df[col].nunique() <= 5]\n",
        "        nominal_cols = [col for col in categorical_cols if df[col].nunique() > 5]\n",
        "\n",
        "        # Display ordinal encoding recommendations\n",
        "        if ordinal_cols:\n",
        "            st.success(\"**Columns recommended for Ordinal Encoding:**\")\n",
        "            st.write(ordinal_cols)\n",
        "        else:\n",
        "            st.warning(\"No columns recommended for ordinal encoding.\")\n",
        "\n",
        "        # Display nominal encoding recommendations\n",
        "        if nominal_cols:\n",
        "            st.success(\"**Columns recommended for Nominal Encoding:**\")\n",
        "            st.write(nominal_cols)\n",
        "        else:\n",
        "            st.warning(\"No columns recommended for nominal encoding.\")\n",
        "\n",
        "        # Identify categorical columns\n",
        "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "        # Identify columns for ordinal and nominal encoding\n",
        "        ordinal_cols = [col for col in categorical_cols if df[col].nunique() <= 5]\n",
        "        nominal_cols = [col for col in categorical_cols if df[col].nunique() > 5]\n",
        "\n",
        "        # Display encoding recommendations and selection\n",
        "        st.success(\"**Recommended columns for encoding:**\")\n",
        "        recommended_cols = ordinal_cols + nominal_cols\n",
        "        if recommended_cols:\n",
        "            selected_cols = st.multiselect(\"Select columns for encoding\", options=recommended_cols, default=recommended_cols)\n",
        "        else:\n",
        "            st.warning(\"No columns recommended for encoding.\")\n",
        "            selected_cols = []\n",
        "\n",
        "        # Execute Ordinal Encoding and Nominal Encoding together\n",
        "        if st.button(\"Apply Selected Encoding and Download Dataset\"):\n",
        "            if selected_cols:\n",
        "                # Separate selected columns for ordinal and nominal encoding\n",
        "                selected_ordinal_cols = [col for col in selected_cols if col in ordinal_cols]\n",
        "                selected_nominal_cols = [col for col in selected_cols if col in nominal_cols]\n",
        "\n",
        "                # Apply Ordinal Encoding\n",
        "                if selected_ordinal_cols:\n",
        "                    encoder = OrdinalEncoder()\n",
        "                    df[selected_ordinal_cols] = encoder.fit_transform(df[selected_ordinal_cols])\n",
        "\n",
        "                # Apply Nominal Encoding (One-Hot Encoding)\n",
        "                if selected_nominal_cols:\n",
        "                    df = pd.get_dummies(df, columns=selected_nominal_cols, drop_first=True)\n",
        "\n",
        "                st.success(\"Encoding applied successfully!\")\n",
        "                st.write(df)\n",
        "\n",
        "                # Create CSV for download\n",
        "                encoded_file = df.to_csv(index=False)\n",
        "                st.download_button(\"Download Encoded Dataset\", data=encoded_file, file_name=\"encoded_dataset.csv\", mime=\"text/csv\")\n",
        "            else:\n",
        "                st.warning(\"No columns selected for encoding.\")\n",
        "\n",
        "    # Feature Scaling Recommendations Tab\n",
        "    with preprocessing_tabs[1]:\n",
        "        st.subheader(\"Feature Scaling Recommendations\")\n",
        "\n",
        "        # Identify numerical columns\n",
        "        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "        # Identify columns for normalization and standardization\n",
        "        normalization_cols = [col for col in numeric_cols if df[col].min() >= 0 and df[col].max() - df[col].min() > 1000]\n",
        "        standardization_cols = [col for col in numeric_cols if df[col].min() < 0 or (df[col].std() > 1 and df[col].max() - df[col].min() <= 1000)]\n",
        "\n",
        "        # Display columns requiring normalization\n",
        "        if normalization_cols:\n",
        "            st.success(\"**Columns requiring normalization (Min-Max Scaling):**\")\n",
        "            st.write(normalization_cols)\n",
        "        else:\n",
        "            st.warning(\"No columns requiring normalization.\")\n",
        "\n",
        "        # Display columns requiring standardization\n",
        "        if standardization_cols:\n",
        "            st.success(\"**Columns requiring standardization (Z-Score):**\")\n",
        "            st.write(standardization_cols)\n",
        "        else:\n",
        "            st.warning(\"No columns requiring standardization.\")\n",
        "\n",
        "        # Identify numerical columns\n",
        "        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "        # Identify columns for normalization and standardization\n",
        "        normalization_cols = [col for col in numeric_cols if df[col].min() >= 0 and df[col].max() - df[col].min() > 1000]\n",
        "        standardization_cols = [col for col in numeric_cols if df[col].min() < 0 or (df[col].std() > 1 and df[col].max() - df[col].min() <= 1000)]\n",
        "\n",
        "        # Display scaling recommendations and selection\n",
        "        st.success(\"**Recommended columns for scaling:**\")\n",
        "        recommended_scaling_cols = normalization_cols + standardization_cols\n",
        "        if recommended_scaling_cols:\n",
        "            selected_scaling_cols = st.multiselect(\"Select columns for scaling\", options=recommended_scaling_cols, default=recommended_scaling_cols)\n",
        "        else:\n",
        "            st.warning(\"No columns recommended for scaling.\")\n",
        "            selected_scaling_cols = []\n",
        "\n",
        "        # Execute Min-Max Scaling and Z-Score Standardization together\n",
        "        if st.button(\"Apply Selected Scaling and Download Dataset\"):\n",
        "            if selected_scaling_cols:\n",
        "                # Apply Min-Max Scaling (Normalization)\n",
        "                scaler_min_max = MinMaxScaler()\n",
        "                scaler_z_score = StandardScaler()\n",
        "\n",
        "                # Apply both scalers\n",
        "                df[selected_scaling_cols] = scaler_min_max.fit_transform(df[selected_scaling_cols])\n",
        "                df[selected_scaling_cols] = scaler_z_score.fit_transform(df[selected_scaling_cols])\n",
        "\n",
        "                st.success(\"Scaling applied successfully!\")\n",
        "                st.write(df)\n",
        "\n",
        "                # Create CSV for download\n",
        "                scaled_file = df.to_csv(index=False)\n",
        "                st.download_button(\"Download Scaled Dataset\", data=scaled_file, file_name=\"scaled_dataset.csv\", mime=\"text/csv\")\n",
        "            else:\n",
        "                st.warning(\"No columns selected for scaling.\")\n",
        "\n",
        "# Tab 3: Supervised Learning\n",
        "with tabs[5]:\n",
        "\n",
        "   # Sub-tabs for Encoding and Feature Scaling\n",
        "    ml_tabs = st.tabs([\"Supervised Learning\",\"Score\", \"Possibility\",\"Download Train and Test Datasets with Results\",\"Future Prediction\",\"Unsupervised Learning\",\"Hyperparameter Tuning\"])\n",
        "\n",
        "    # Encoding Recommendations Tab\n",
        "    with ml_tabs[0]:\n",
        "        st.subheader(\"Supervised Learning\")\n",
        "\n",
        "        # Target column selection\n",
        "        target_col = st.selectbox(\"Select the target column:\", [None] + list(df.columns))\n",
        "        if target_col:\n",
        "            y = df[target_col]\n",
        "            unique_values = y.nunique()\n",
        "\n",
        "            # Determine if the target is continuous or discrete\n",
        "            if y.dtype in [np.int64, np.float64]:\n",
        "                if unique_values > 20:\n",
        "                    inferred_task_type = \"Regression\"\n",
        "                    target_status = \"Continuous\"\n",
        "                else:\n",
        "                    inferred_task_type = \"Classification\"\n",
        "                    target_status = \"Discrete\"\n",
        "            else:\n",
        "                inferred_task_type = \"Classification\"\n",
        "                target_status = \"Discrete\"\n",
        "\n",
        "            # Display inferred target properties\n",
        "            st.success(f\"**Target Column Data Type:** {target_status}\")\n",
        "            st.success(f\"**Inferred Task Type:** {inferred_task_type}\")\n",
        "\n",
        "            # Check for balance in classification tasks\n",
        "            if inferred_task_type == \"Classification\":\n",
        "                class_distribution = y.value_counts(normalize=True)\n",
        "                max_class_proportion = class_distribution.max()\n",
        "                balance_status = \"Balanced\" if max_class_proportion <= 0.7 else \"Unbalanced\"\n",
        "                st.info(f\"**Dataset Balance Status:** {balance_status}\")\n",
        "                st.write(\"**Class Distribution:**\")\n",
        "                st.bar_chart(class_distribution)\n",
        "\n",
        "                st.info(\n",
        "                    \"Oversampling: - Add Extra Data\\n\"\n",
        "                    \"1) Adds new samples of the minority class\\n\"\n",
        "                    \"2) Used when there is not enough data\\n\"\n",
        "                    \"3) Can be effective in small datasets\\n\"\n",
        "                    \"4) Can lead to overfitting\\n\"\n",
        "                    \"5) A popular technique is SMOTE (Synthetic Minority Over-sampling Technique)\"\n",
        "                )\n",
        "                st.warning(\"Oversampling:\\n\"\n",
        "                           \"Oversampling can create artificial class distributions that are not representative of the real world\"\n",
        "                           )\n",
        "                st.info(\n",
        "                    \"Undersampling: - Remove Extra Data\\n\"\n",
        "                    \"1) Adds new samples of the majority class\\n\"\n",
        "                    \"2) Used when there is too much data\\n\"\n",
        "                    \"3) Can be effective in large datasets\\n\"\n",
        "                    \"4) Can lead to loss of information\\n\"\n",
        "                    \"5) Common methods include cluster centroids and Tomek links\"\n",
        "                )\n",
        "                st.warning(\"Undersampling:\\n\"\n",
        "                           \"Undersampling can lead to biased models because it can cause loss of information.\"\n",
        "                           )\n",
        "\n",
        "            # Manual override for task type\n",
        "            manual_task_type = st.radio(\n",
        "                \"Select the problem type manually:\",\n",
        "                options=[\"Classification\", \"Regression\"],\n",
        "                index=0 if inferred_task_type == \"Classification\" else 1,\n",
        "            )\n",
        "            manual_data_type = st.radio(\n",
        "                \"Select the target type manually:\",\n",
        "                options=[\"Continuous\", \"Discrete\"],\n",
        "                index=0 if target_status == \"Continuous\" else 1,\n",
        "            )\n",
        "\n",
        "            # Display suggested algorithms based on manual selection\n",
        "            if manual_task_type == \"Regression\":\n",
        "                st.write(\"**You selected Regression.**\")\n",
        "                st.write(\"**Suggested Regression Algorithms:**\")\n",
        "                if manual_data_type == \"Continuous\":\n",
        "                    st.write(\"- Linear Regression\")\n",
        "                    st.write(\"- Polynomial Regression\")\n",
        "                    st.write(\"- Ridge Regression\")\n",
        "                    st.write(\"- Lasso Regression\")\n",
        "                    st.write(\"- SVM Regressor\")\n",
        "                else:\n",
        "                    st.write(\"- Decision Tree Regressor\")\n",
        "                    st.write(\"- Random Forest Regressor\")\n",
        "                    st.write(\"- Boosting Regressor (e.g., XGBoost)\")\n",
        "                    st.write(\"- KNN Regressor\")\n",
        "                    st.write(\"- SVM Regressor\")\n",
        "\n",
        "            elif manual_task_type == \"Classification\":\n",
        "                st.write(\"**You selected Classification.**\")\n",
        "                if unique_values == 2:\n",
        "                    st.success(\"**Suggested Binary Classification Algorithms:**\")\n",
        "                    st.write(\"- Logistic Regression\")\n",
        "                    st.write(\"- SVM Classifier\")\n",
        "                else:\n",
        "                    st.success(\"**Suggested Multiclass Classification Algorithms:**\")\n",
        "                    st.write(\"- Decision Tree Classifier\")\n",
        "                    st.write(\"- Random Forest Classifier\")\n",
        "                    st.write(\"- Boosting Classifier (e.g., XGBoost)\")\n",
        "                    st.write(\"- KNN Classifier\")\n",
        "                    st.write(\"- Naive Bayes Classifier\")\n",
        "\n",
        "            # Feature selection\n",
        "            features = st.multiselect(\"Select feature columns\", options=[col for col in df.columns if col != target_col])\n",
        "            if features:\n",
        "                X = df[features]\n",
        "                y = df[target_col]\n",
        "\n",
        "                # Map binary target column for classification\n",
        "                if manual_task_type == \"Classification\" and unique_values == 2:\n",
        "                    y = y.map({y.unique()[0]: 0, y.unique()[1]: 1})\n",
        "\n",
        "                # Train-test split\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "                # Algorithm selection\n",
        "                algorithm = st.selectbox(\n",
        "                    \"Select Algorithm\",\n",
        "                    [\n",
        "                        \"Linear Regression\", \"Polynomial Regression\", \"Ridge Regression\", \"Lasso Regression\",\n",
        "                        \"ElasticNet Regression\", \"SVM Regression\", \"Decision Tree Regressor\",\n",
        "                        \"Random Forest Regressor\", \"Boosting Regressor\", \"KNN Regressor\"\n",
        "                    ] if manual_task_type == 'Regression' else [\n",
        "                        \"Logistic Regression\", \"SVM Classifier\", \"Decision Tree Classifier\",\n",
        "                        \"Random Forest Classifier\", \"Boosting Classifier\", \"KNN Classifier\",\n",
        "                        \"Naive Bayes Classifier\"\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                # Initialize model\n",
        "                model = None\n",
        "                if algorithm == \"Linear Regression\":\n",
        "                    model = LinearRegression()\n",
        "                elif algorithm == \"Polynomial Regression\":\n",
        "                    degree = st.slider(\"Select Degree for Polynomial Regression\", min_value=2, max_value=10, value=2)\n",
        "                    model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression())\n",
        "                elif algorithm == \"Ridge Regression\":\n",
        "                    alpha = st.slider(\"Select Alpha for Ridge Regression\", min_value=0.01, max_value=10.0, value=1.0)\n",
        "                    model = Ridge(alpha=alpha)\n",
        "                elif algorithm == \"Lasso Regression\":\n",
        "                    alpha = st.slider(\"Select Alpha for Lasso Regression\", min_value=0.01, max_value=10.0, value=1.0)\n",
        "                    model = Lasso(alpha=alpha)\n",
        "                elif algorithm == \"Decision Tree Regressor\":\n",
        "                    model = DecisionTreeRegressor()\n",
        "                elif algorithm == \"Random Forest Regressor\":\n",
        "                    model = RandomForestRegressor()\n",
        "                elif algorithm == \"Boosting Regressor\":\n",
        "                    model = XGBRegressor()\n",
        "                elif algorithm == \"KNN Regressor\":\n",
        "                    model = KNeighborsRegressor()\n",
        "                elif algorithm == \"Logistic Regression\":\n",
        "                    model = LogisticRegression()\n",
        "                elif algorithm == \"SVM Classifier\":\n",
        "                    model = SVC()\n",
        "                elif algorithm == \"Decision Tree Classifier\":\n",
        "                    model = DecisionTreeClassifier()\n",
        "                elif algorithm == \"Random Forest Classifier\":\n",
        "                    model = RandomForestClassifier()\n",
        "                elif algorithm == \"Boosting Classifier\":\n",
        "                    model = XGBClassifier()\n",
        "                elif algorithm == \"KNN Classifier\":\n",
        "                    model = KNeighborsClassifier()\n",
        "                elif algorithm == \"Naive Bayes Classifier\":\n",
        "                    model = GaussianNB()\n",
        "\n",
        "                # Train and evaluate the model\n",
        "                start_time = time.time()\n",
        "                model.fit(X_train, y_train)\n",
        "                train_pred = model.predict(X_train)\n",
        "                test_pred = model.predict(X_test)\n",
        "                end_time = time.time()\n",
        "\n",
        "                # Display training time\n",
        "                st.write(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "                # Model fitting and prediction\n",
        "                start_time = time.time()\n",
        "                model.fit(X_train, y_train)\n",
        "                train_pred = model.predict(X_train)\n",
        "                test_pred = model.predict(X_test)\n",
        "                end_time = time.time()\n",
        "\n",
        "                # Display training time\n",
        "                st.write(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "                # User choice for plot data source: Train or Test\n",
        "                data_source = st.radio(\"Select data for visualization:\", [\"Train Data\", \"Test Data\"])\n",
        "                if data_source == \"Train Data\":\n",
        "                    X_plot, y_actual, y_pred = X_train, y_train, train_pred\n",
        "                else:\n",
        "                    X_plot, y_actual, y_pred = X_test, y_test, test_pred\n",
        "\n",
        "                # Apply PCA for dimensionality reduction if features > 2\n",
        "                if X_plot.shape[1] > 2:\n",
        "                    st.warning(\"Data has more than 2 features. Reducing dimensions using PCA...\")\n",
        "                    pca = PCA(n_components=2)\n",
        "                    X_plot_2d = pca.fit_transform(X_plot)\n",
        "                else:\n",
        "                    X_plot_2d = X_plot.values\n",
        "\n",
        "                # Visualization for Regression Algorithms\n",
        "                if manual_task_type == \"Regression\":\n",
        "                    fig, ax = plt.subplots()\n",
        "\n",
        "                    # Scatter plot for actual vs predicted\n",
        "                    sns.scatterplot(x=y_actual, y=y_pred, ax=ax, color='blue', label=\"Predicted Points\")\n",
        "\n",
        "                    # Best fit line\n",
        "                    min_val, max_val = min(y_actual.min(), y_pred.min()), max(y_actual.max(), y_pred.max())\n",
        "                    ax.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label=\"Best Fit Line\")\n",
        "\n",
        "                    ax.set_title(f\"Actual vs Predicted ({data_source}) - {algorithm}\")\n",
        "                    ax.set_xlabel(\"Actual Values\")\n",
        "                    ax.set_ylabel(\"Predicted Values\")\n",
        "                    ax.legend()\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "                    # Show evaluation metrics\n",
        "                    st.write(f\"Mean Squared Error: {mean_squared_error(y_actual, y_pred):.4f}\")\n",
        "\n",
        "                # Visualization for Classification Algorithms\n",
        "                elif manual_task_type == \"Classification\":\n",
        "                   acc = accuracy_score(y_actual, y_pred)\n",
        "                   st.write(f\"Accuracy Score: {acc:.4f}\")\n",
        "\n",
        "                   # Decision boundary plot (now with PCA if needed)\n",
        "                   fig, ax = plt.subplots()\n",
        "                   x_min, x_max = X_plot_2d[:, 0].min() - 1, X_plot_2d[:, 0].max() + 1\n",
        "                   y_min, y_max = X_plot_2d[:, 1].min() - 1, X_plot_2d[:, 1].max() + 1\n",
        "                   xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "\n",
        "                   # Retrain model for boundary plot\n",
        "                   model_2d = model.__class__()  # Create a new instance of the classifier\n",
        "                   model_2d.fit(X_plot_2d, y_actual)\n",
        "\n",
        "                   Z = model_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "                   Z = Z.reshape(xx.shape)\n",
        "\n",
        "                   ax.contourf(xx, yy, Z, alpha=0.3)\n",
        "                   sns.scatterplot(x=X_plot_2d[:, 0], y=X_plot_2d[:, 1], hue=y_actual, ax=ax)\n",
        "                   ax.set_title(f\"Decision Boundary for {algorithm} ({data_source})\")\n",
        "                   st.pyplot(fig)\n",
        "\n",
        "\n",
        "   # Score Recommendations Tab\n",
        "with ml_tabs[1]:\n",
        "    st.subheader(\"Score\")\n",
        "\n",
        "    # Compute and display metrics for both train and test sets\n",
        "    if manual_task_type == \"Classification\":\n",
        "        for dataset, y_true, y_pred in [(\"Train\", y_train, train_pred), (\"Test\", y_test, test_pred)]:\n",
        "            st.subheader(f\"{dataset} Metrics\")\n",
        "            accuracy = accuracy_score(y_true, y_pred)\n",
        "            precision = precision_score(y_true, y_pred, average='macro')\n",
        "            recall = recall_score(y_true, y_pred, average='macro')\n",
        "            f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "            st.write(f\"Accuracy: {accuracy:.4f}\")\n",
        "            st.write(f\"Precision: {precision:.4f}\")\n",
        "            st.write(f\"Recall: {recall:.4f}\")\n",
        "            st.write(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "        # Analyze the fit status based on train and test accuracy\n",
        "        train_accuracy = accuracy_score(y_train, train_pred)\n",
        "        test_accuracy = accuracy_score(y_test, test_pred)\n",
        "\n",
        "        st.subheader(\"Model Fit Status\")\n",
        "        fit_status = \"Generalizing Well\"\n",
        "\n",
        "        if train_accuracy > test_accuracy + 0.1:\n",
        "            fit_status = \"Overfit: Train accuracy is much higher than Test accuracy.\"\n",
        "        elif train_accuracy < test_accuracy - 0.1:\n",
        "            fit_status = \"Underfit: Model performs poorly on both Train and Test data.\"\n",
        "        else:\n",
        "            fit_status = \"Good Fit: Model generalizes well to new data.\"\n",
        "\n",
        "        st.warning(f\"**Status:** {fit_status}\")\n",
        "\n",
        "        st.info(\"Precision = TP/(TP + FP)\")\n",
        "        st.info(\"Recall = TP/(TP + FN)\")\n",
        "\n",
        "        # Display classification report as a table for test set\n",
        "        st.subheader(\"Classification Report (Test Data)\")\n",
        "        report = classification_report(y_test, test_pred, output_dict=True)\n",
        "        report_df = pd.DataFrame(report).transpose()\n",
        "        st.table(report_df.style.format(precision=4))\n",
        "\n",
        "        # Plot confusion matrix for test set\n",
        "        st.subheader(\"Confusion Matrix (Test Data)\")\n",
        "        cm = confusion_matrix(y_test, test_pred)\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        # Annotate the confusion matrix with additional labels\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Predicted 0\", \"Predicted 1\"], yticklabels=[\"Actual 0\", \"Actual 1\"])\n",
        "        ax.set_xlabel(\"Predicted Labels\")\n",
        "        ax.set_ylabel(\"True Labels\")\n",
        "        ax.set_title(\"Confusion Matrix with Labels\")\n",
        "\n",
        "        # Extract TP, TN, FP, FN values\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        st.write(f\"**True Positive (TP): {tp}** - Predicted 1 and Actual 1\")\n",
        "        st.write(f\"**True Negative (TN): {tn}** - Predicted 0 and Actual 0\")\n",
        "        st.write(f\"**False Positive (FP): {fp}** - Predicted 1 but Actual 0\")\n",
        "        st.write(f\"**False Negative (FN): {fn}** - Predicted 0 but Actual 1\")\n",
        "\n",
        "        # Display the heatmap\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        st.markdown(\n",
        "            \"\"\"\n",
        "            <b>True Positive</b>: The test correctly predicts a positive outcome when the actual outcome is positive.<br>\n",
        "            <b>True Negative</b>: The test correctly predicts a negative outcome when the actual outcome is negative.<br>\n",
        "            <b>False Positive</b>: The test incorrectly predicts a positive outcome when the actual outcome is negative.<br>\n",
        "            <b>False Negative</b>: The test incorrectly predicts a negative outcome when the actual outcome is positive.\n",
        "            \"\"\",\n",
        "            unsafe_allow_html=True\n",
        "        )\n",
        "\n",
        "        # Check if the task is classification\n",
        "        if manual_task_type == \"Classification\":\n",
        "            # Add a column to indicate TP, TN, FP, FN\n",
        "            result_labels = []\n",
        "            for true, pred in zip(y_test, y_pred):\n",
        "                if true == 1 and pred == 1:\n",
        "                    result_labels.append(\"True Positive\")\n",
        "                elif true == 0 and pred == 0:\n",
        "                    result_labels.append(\"True Negative\")\n",
        "                elif true == 0 and pred == 1:\n",
        "                    result_labels.append(\"False Positive\")\n",
        "                elif true == 1 and pred == 0:\n",
        "                    result_labels.append(\"False Negative\")\n",
        "\n",
        "            # Add this result to the test data for download\n",
        "            test_data_with_results = X_test.copy()\n",
        "            test_data_with_results[target_col] = y_test\n",
        "            test_data_with_results[\"Predicted\"] = y_pred\n",
        "            test_data_with_results[\"Result\"] = result_labels\n",
        "\n",
        "            # Check the shape of the data\n",
        "            print(test_data_with_results.shape)\n",
        "            print(test_data_with_results.head())\n",
        "\n",
        "            from sklearn.metrics import roc_curve, auc\n",
        "            import matplotlib.pyplot as plt\n",
        "            import seaborn as sns\n",
        "            import numpy as np\n",
        "\n",
        "            # Assuming you have trained your model and obtained predictions\n",
        "\n",
        "            # Get predicted probabilities for the positive class (1)\n",
        "            if manual_task_type == \"Classification\" and algorithm not in [\"Naive Bayes Classifier\"]:\n",
        "                y_pred_prob = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
        "\n",
        "                # Compute ROC curve and AUC\n",
        "                fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "                roc_auc = auc(fpr, tpr)\n",
        "\n",
        "                # Plot ROC curve\n",
        "                fig, ax = plt.subplots(figsize=(8, 6))\n",
        "                ax.plot(fpr, tpr, color='b', label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "                ax.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random classifier line\n",
        "                ax.set_xlabel('False Positive Rate')\n",
        "                ax.set_ylabel('True Positive Rate')\n",
        "                ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
        "                ax.legend(loc='lower right')\n",
        "\n",
        "                # Show the plot\n",
        "                st.pyplot(fig)\n",
        "\n",
        "                # Display detailed explanation for ROC and AUC\n",
        "                st.markdown(\n",
        "                \"\"\"\n",
        "                **ROC Curve**: The ROC curve represents the performance of a classification model at all classification thresholds. It is a plot of the True Positive Rate (TPR) against the False Positive Rate (FPR).<br>\n",
        "                - **True Positive Rate (TPR)**: Also known as Recall, it is the fraction of actual positives that are correctly identified by the model.<br>\n",
        "                - **False Positive Rate (FPR)**: The fraction of actual negatives that are incorrectly classified as positives.<br>\n",
        "                - **Diagonal line (Random Classifier)**: The line represents the performance of a random classifier. A good classifier should have its ROC curve above this line.\n",
        "\n",
        "                **AUC (Area Under the Curve)**: AUC quantifies the overall ability of the model to discriminate between positive and negative cases.<br>\n",
        "                - An AUC of 0.5 means the model is no better than random guessing.<br>\n",
        "                - An AUC of 1.0 indicates a perfect model.\n",
        "                \"\"\"\n",
        "                )\n",
        "\n",
        "                # Option to adjust thresholds\n",
        "                threshold_slider = st.slider(\"Adjust Threshold for Classification\", min_value=0.0, max_value=1.0, value=0.5, step=0.05)\n",
        "                y_pred_adjusted = (y_pred_prob >= threshold_slider).astype(int)\n",
        "\n",
        "                # Show adjusted accuracy, precision, recall, F1 score after changing threshold\n",
        "                accuracy_adjusted = accuracy_score(y_test, y_pred_adjusted)\n",
        "                precision_adjusted = precision_score(y_test, y_pred_adjusted, average='macro')\n",
        "                recall_adjusted = recall_score(y_test, y_pred_adjusted, average='macro')\n",
        "                f1_adjusted = f1_score(y_test, y_pred_adjusted, average='macro')\n",
        "\n",
        "                st.write(f\"**Adjusted Metrics at Threshold {threshold_slider}:**\")\n",
        "                st.write(f\"Accuracy: {accuracy_adjusted:.4f}\")\n",
        "                st.write(f\"Precision: {precision_adjusted:.4f}\")\n",
        "                st.write(f\"Recall: {recall_adjusted:.4f}\")\n",
        "                st.write(f\"F1 Score: {f1_adjusted:.4f}\")\n",
        "\n",
        "                # Optionally, you can also display confusion matrix for the adjusted threshold\n",
        "                cm_adjusted = confusion_matrix(y_test, y_pred_adjusted)\n",
        "                fig, ax = plt.subplots(figsize=(6, 6))\n",
        "                sns.heatmap(cm_adjusted, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Predicted 0\", \"Predicted 1\"], yticklabels=[\"Actual 0\", \"Actual 1\"])\n",
        "                ax.set_xlabel(\"Predicted Labels\")\n",
        "                ax.set_ylabel(\"True Labels\")\n",
        "                ax.set_title(f\"Confusion Matrix at Threshold {threshold_slider}\")\n",
        "                st.pyplot(fig)\n",
        "\n",
        "            # Provide a download link\n",
        "            st.subheader(\"Download Updated Dataset\")\n",
        "            buffer = io.BytesIO()\n",
        "            test_data_with_results.to_csv(buffer, index=False)\n",
        "            buffer.seek(0)\n",
        "            buffer.flush()  # Ensure everything is written to the buffer\n",
        "\n",
        "            st.write(\"Shape of dataset:\", df.shape)\n",
        "            st.success(\"downloaded data is test data\")\n",
        "            st.download_button(\n",
        "                label=\"Download Dataset with Results (CSV)\",\n",
        "                data=buffer,\n",
        "                file_name=\"updated_dataset_with_results.csv\",\n",
        "                mime=\"text/csv\",\n",
        "            )\n",
        "\n",
        "    elif manual_task_type == \"Regression\":\n",
        "        # Calculate MSE for test data\n",
        "        mse = mean_squared_error(y_test, test_pred)\n",
        "        st.info(f\"Mean Squared Error (MSE) on Test Data: {mse:.4f}\")\n",
        "\n",
        "        # Visualize MSE\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.scatter(y_test, test_pred, color='blue', label='Predicted vs Actual')\n",
        "        ax.plot(y_test, y_test, color='red', label='Perfect Fit Line')\n",
        "        mse_errors = (y_test - test_pred) ** 2\n",
        "        ax.bar(range(len(mse_errors)), mse_errors, color='gray', alpha=0.3, label='Squared Errors')\n",
        "        ax.legend()\n",
        "        ax.set_title(\"MSE Visualization\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Calculate MAE for test data\n",
        "        mae = mean_absolute_error(y_test, test_pred)\n",
        "        st.info(f\"Mean Absolute Error (MAE) on Test Data: {mae:.4f}\")\n",
        "\n",
        "        # Visualize MAE\n",
        "        fig, ax = plt.subplots()\n",
        "        abs_errors = abs(y_test - test_pred)\n",
        "        ax.bar(range(len(abs_errors)), abs_errors, color='orange', alpha=0.6, label='Absolute Errors')\n",
        "        ax.legend()\n",
        "        ax.set_title(\"MAE Visualization\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Calculate R² score for test data\n",
        "        r2_test = r2_score(y_test, test_pred)\n",
        "        st.info(f\"R² Score on Test Data: {r2_test:.4f}\")\n",
        "\n",
        "        # Visualize R² for test data\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.scatter(y_test, test_pred, color='blue', label='Predicted vs Actual')\n",
        "        ax.plot(y_test, y_test, color='red', label='Perfect Fit Line')\n",
        "        ax.legend()\n",
        "        ax.set_title(\"R² Score Visualization (Test Data)\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Calculate R² score for training data\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        r2_train = r2_score(y_train, y_train_pred)\n",
        "        st.info(f\"R² Score on Training Data: {r2_train:.4f}\")\n",
        "\n",
        "        # Visualize R² for training data\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.scatter(y_train, y_train_pred, color='blue', label='Predicted vs Actual')\n",
        "        ax.plot(y_train, y_train, color='red', label='Perfect Fit Line')\n",
        "        ax.legend()\n",
        "        ax.set_title(\"R² Score Visualization (Training Data)\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Calculate Adjusted R² for test data\n",
        "        n = len(y_test)  # number of observations\n",
        "        p = X_test.shape[1]  # number of predictors\n",
        "        adjusted_r2 = 1 - ((1 - r2_test) * (n - 1) / (n - p - 1))\n",
        "        st.success(f\"Adjusted R² Score on Test Data: {adjusted_r2:.4f}\")\n",
        "\n",
        "        # Visualize Adjusted R² impact\n",
        "        fig, ax = plt.subplots()\n",
        "        predictors = np.arange(1, p + 5)\n",
        "        adjusted_r2_values = [1 - (1 - r2_test) * ((n - 1) / (n - p_temp - 1)) for p_temp in predictors]\n",
        "        ax.plot(predictors, adjusted_r2_values, marker='o', label='Adjusted R²')\n",
        "        ax.axhline(y=r2_test, color='red', linestyle='--', label='R² Score')\n",
        "        ax.set_xlabel('Number of Predictors')\n",
        "        ax.set_ylabel('R² Values')\n",
        "        ax.set_title(\"Adjusted R² vs Number of Predictors\")\n",
        "        ax.legend()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Fit the model on training data\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Get the R² score for both training and test data\n",
        "        r2_train = model.score(X_train, y_train)\n",
        "        r2_test = model.score(X_test, y_test)\n",
        "\n",
        "        # Calculate Adjusted R² for both training and test data\n",
        "        n_train = len(y_train)  # number of training observations\n",
        "        n_test = len(y_test)    # number of test observations\n",
        "        p = X_train.shape[1]    # number of predictors\n",
        "\n",
        "        # Adjusted R² for training data\n",
        "        adjusted_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p - 1))\n",
        "\n",
        "        # Adjusted R² for test data\n",
        "        adjusted_r2_test = 1 - ((1 - r2_test) * (n_test - 1) / (n_test - p - 1))\n",
        "\n",
        "        # Display the results\n",
        "        st.success(f\"Adjusted R² Score on Training Data: {adjusted_r2_train:.4f}\")\n",
        "        st.success(f\"Adjusted R² Score on Test Data: {adjusted_r2_test:.4f}\")\n",
        "\n",
        "        # Display training and test accuracy comparison\n",
        "        st.subheader(\"Training vs Test Accuracy (R² Score)\")\n",
        "        st.write(f\"**Training R² Score:** {r2_train:.4f}\")\n",
        "        st.write(f\"**Test R² Score:** {r2_test:.4f}\")\n",
        "\n",
        "        # Interpretation of training vs test accuracy\n",
        "        if r2_train > r2_test:\n",
        "            st.error(\"The model may be overfitting, as the training accuracy is significantly higher than the test accuracy.\")\n",
        "        elif r2_train < r2_test:\n",
        "            st.error(\"The model may be underfitting, as the training accuracy is lower than the test accuracy.\")\n",
        "        else:\n",
        "            st.success(\"The model generalizes well, as the training and test accuracies are similar.\")\n",
        "\n",
        "        # Display training and test accuracy comparison\n",
        "        st.subheader(\"Training vs Test Accuracy (Adjusted R² Score)\")\n",
        "        st.write(f\"**Training Adjusted R² Score:** {adjusted_r2_train:.4f}\")\n",
        "        st.write(f\"**Test Adjusted R² Score:** {adjusted_r2_test:.4f}\")\n",
        "\n",
        "        # Interpretation of training vs test accuracy\n",
        "        if adjusted_r2_train > adjusted_r2_test:\n",
        "            st.error(\"The model may be overfitting, as the training accuracy is significantly higher than the test accuracy.\")\n",
        "        elif adjusted_r2_train < adjusted_r2_test:\n",
        "            st.error(\"The model may be underfitting, as the training accuracy is lower than the test accuracy.\")\n",
        "        else:\n",
        "            st.success(\"The model generalizes well, as the training and test accuracies are similar.\")\n",
        "\n",
        "        # MSE Explanation\n",
        "        with st.success(\"### Mean Squared Error (MSE) Explanation\"):\n",
        "            st.success(\"\"\"\n",
        "            **Definition:**\n",
        "            Mean Squared Error (MSE) measures the average squared difference between actual and predicted values.\n",
        "\n",
        "            **Key Points:**\n",
        "            - A lower MSE indicates a better model fit.\n",
        "            - MSE penalizes large errors more than small ones because of the squaring operation.\n",
        "            - The unit of MSE is the square of the target variable's unit.\n",
        "            - The square root of MSE is called Root Mean Squared Error (RMSE).\n",
        "            \"\"\")\n",
        "\n",
        "        # MAE Explanation\n",
        "        with st.success(\"### Mean Absolute Error (MAE) Explanation\"):\n",
        "            st.success(\"\"\"\n",
        "            **Definition:**\n",
        "            Mean Absolute Error (MAE) calculates the average of the absolute differences between actual and predicted values.\n",
        "\n",
        "            **Key Points:**\n",
        "            - A lower MAE indicates better accuracy.\n",
        "            - Unlike MSE, MAE treats all errors equally without squaring.\n",
        "            - MAE has the same unit as the target variable, making it easier to interpret.\n",
        "            - Less sensitive to outliers compared to MSE.\n",
        "            \"\"\")\n",
        "\n",
        "        # R² Score Explanation\n",
        "        with st.success(\"### R² Score (Coefficient of Determination) Explanation\"):\n",
        "            st.success(\"\"\"\n",
        "            **Definition:**\n",
        "            R² measures how well the model explains the variance in the target variable.\n",
        "\n",
        "            **Key Points:**\n",
        "            - R² ranges from 0 to 1. Higher R² indicates better model fit.\n",
        "            - An R² of 0 means the model performs no better than predicting the mean.\n",
        "            - A negative R² can occur when the model performs worse than predicting the mean.\n",
        "            - An R² of 1 means perfect predictions.\n",
        "            \"\"\")\n",
        "\n",
        "        # Adjusted R² Explanation\n",
        "        with st.success(\"### Adjusted R² Score Explanation\"):\n",
        "            st.success(\"\"\"\n",
        "            **Definition:**\n",
        "            Adjusted R² penalizes the addition of unnecessary predictors in the model and accounts for the number of features.\n",
        "\n",
        "            **Formula:**\n",
        "            Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - p - 1)]\n",
        "            where:\n",
        "            - n = number of observations\n",
        "            - p = number of predictors\n",
        "\n",
        "            **Key Points:**\n",
        "            - Adjusted R² is always less than or equal to R².\n",
        "            - It increases only if the new predictor improves the model more than would be expected by chance.\n",
        "            - It is useful for comparing models with different numbers of predictors.\n",
        "            \"\"\")\n",
        "\n",
        "        st.write(\"MSE for Regression Tasks\")\n",
        "        st.write(\"The range of values for mean squared error (MSE) is zero to infinity (\\(0\\le \\text{MSE}<\\infty \\)). A lower MSE value indicates a better model.\")\n",
        "        st.write(\"The square root of MSE is called root mean square error (RMSE)\")\n",
        "        st.write(\"the Mean Square Error (MSE) is a crucial metric for evaluating the performance of predictive models. It measures the average squared difference between the predicted and the actual target values within a dataset.\")\n",
        "    else:\n",
        "        st.warning(\"Please select a target column to proceed.\")\n",
        "\n",
        "    # machine learning Recommendations Tab\n",
        "with ml_tabs[2]:\n",
        "        st.subheader(\"Possibilites\")\n",
        "\n",
        "        if features:\n",
        "                X = df[features]\n",
        "                y = df[target_col]\n",
        "\n",
        "                # Map binary target column for classification\n",
        "                if inferred_task_type == \"Classification\" and unique_values == 2:\n",
        "                    y = y.map({y.unique()[0]: 0, y.unique()[1]: 1})\n",
        "\n",
        "                # Cross-Validation Section\n",
        "                st.subheader(\"Cross-Validation\")\n",
        "\n",
        "                st.write(\"Shape of dataset:\", df.shape)\n",
        "\n",
        "                # Options for Cross-Validation Methods\n",
        "                cv_methods = {\n",
        "                    \"K-Fold\": KFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                    \"Stratified K-Fold\": StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                    \"Holdout (80/20 Split)\": None,\n",
        "                    \"Leave-One-Out\": LeaveOneOut(),\n",
        "                    \"Leave-P-Out (P=2)\": LeavePOut(p=2)\n",
        "                }\n",
        "\n",
        "                # Manual selection of cross-validation method with a unique key\n",
        "                selected_method = st.selectbox(\n",
        "                    \"Select Cross-Validation Method\",\n",
        "                    list(cv_methods.keys()),\n",
        "                    key=\"cv_method_selector\"\n",
        "                )\n",
        "\n",
        "                # Options for K-Fold cross-validation to select number of folds\n",
        "                if selected_method == \"K-Fold\":\n",
        "                    n_splits = st.slider(\"Select the number of folds:\", min_value=2, max_value=10, value=5, key=\"kfold_slider\")\n",
        "                    cv_methods[\"K-Fold\"] = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "                # Options for Holdout to set test size\n",
        "                if selected_method == \"Holdout (80/20 Split)\":\n",
        "                    test_size = st.slider(\"Select the test size percentage:\", min_value=0.1, max_value=0.9, value=0.2, key=\"holdout_test_size\")\n",
        "\n",
        "                # Options for Stratified K-Fold to set the number of splits\n",
        "                if selected_method == \"Stratified K-Fold\":\n",
        "                    unique_classes = y.nunique()  # Get the number of unique classes in the target column\n",
        "\n",
        "                    # Dynamically set max_splits to ensure it doesn't exceed the number of unique classes\n",
        "                    min_splits = 2\n",
        "                    max_splits = min(unique_classes, 10)  # Ensure max splits don't exceed unique classes\n",
        "\n",
        "                    # Display recommendation for number of splits\n",
        "                    st.write(f\"Based on the number of unique values in the target column ({unique_classes} unique values), \"\n",
        "                             f\"we recommend a maximum of {max_splits} splits.\")\n",
        "\n",
        "                    # Allow the user to select the number of splits within the calculated range\n",
        "                    n_splits_stratified = st.slider(\n",
        "                        f\"Select the number of splits for Stratified K-Fold (max {max_splits} based on unique classes):\",\n",
        "                        min_value=min_splits,\n",
        "                        max_value=max_splits,\n",
        "                        value=5,\n",
        "                        key=\"stratified_kfold_slider\"\n",
        "                    )\n",
        "\n",
        "                    # Add a custom class distribution option for imbalanced datasets\n",
        "                    if unique_values == 2:  # For binary classification\n",
        "                        st.subheader(\"Custom Class Distribution\")\n",
        "\n",
        "                        class_1_ratio = st.slider(\n",
        "                            \"Select the ratio for Class 1 (e.g., 75% for Class 1 and 25% for Class 2):\",\n",
        "                            min_value=50,\n",
        "                            max_value=90,\n",
        "                            value=75,\n",
        "                            step=5,\n",
        "                            key=\"class_1_ratio_slider\"\n",
        "                        )\n",
        "\n",
        "                        class_2_ratio = 100 - class_1_ratio  # Automatically calculate Class 2 ratio\n",
        "\n",
        "                        st.write(f\"Class 1 will have {class_1_ratio}% of the samples, and Class 2 will have {class_2_ratio}%.\")\n",
        "\n",
        "                        # Store the custom class ratio in the Stratified K-Fold parameters\n",
        "                        cv_methods[\"Stratified K-Fold\"] = StratifiedKFold(\n",
        "                            n_splits=n_splits_stratified,\n",
        "                            shuffle=True,\n",
        "                            random_state=42\n",
        "                        )\n",
        "\n",
        "                        # Now apply this distribution for data splitting during Stratified K-Fold\n",
        "                        # Displaying the stratified split for the user\n",
        "                        st.write(f\"Applying Stratified K-Fold with {class_1_ratio}% Class 1 and {class_2_ratio}% Class 2.\")\n",
        "\n",
        "                    else:\n",
        "                        cv_methods[\"Stratified K-Fold\"] = StratifiedKFold(\n",
        "                            n_splits=n_splits_stratified,\n",
        "                            shuffle=True,\n",
        "                            random_state=42\n",
        "                        )\n",
        "\n",
        "\n",
        "\n",
        "                # Perform selected cross-validation\n",
        "                if selected_method == \"Holdout (80/20 Split)\":\n",
        "                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "                    model.fit(X_train, y_train)\n",
        "                    train_accuracy = model.score(X_train, y_train)\n",
        "                    test_accuracy = model.score(X_test, y_test)\n",
        "                    st.write(f\"**Train Accuracy:** {train_accuracy:.4f}\")\n",
        "                    st.write(f\"**Test Accuracy:** {test_accuracy:.4f}\")\n",
        "\n",
        "                else:\n",
        "                    cv = cv_methods[selected_method]\n",
        "                    scores = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\" if inferred_task_type == \"Classification\" else \"r2\")\n",
        "                    train_accuracy = model.fit(X, y).score(X, y)  # Training score using all data for comparison\n",
        "                    test_accuracy = np.mean(scores)\n",
        "\n",
        "                    st.write(f\"**Cross-Validation Accuracy ({selected_method}):** {test_accuracy:.4f} ± {np.std(scores):.4f}\")\n",
        "                    st.write(f\"**Train Accuracy:** {train_accuracy:.4f}\")\n",
        "\n",
        "                    # Analyze model fit status\n",
        "                    fit_status = \"Generalizing Well\"\n",
        "                    if train_accuracy > test_accuracy + 0.1:\n",
        "                        fit_status = \"Overfit\"\n",
        "                    elif train_accuracy < test_accuracy - 0.1:\n",
        "                        fit_status = \"Underfit\"\n",
        "\n",
        "                    # Display the fit status\n",
        "                    st.warning(f\"**Model Fit Status:** {fit_status}\")\n",
        "\n",
        "                # Summary Recommendation\n",
        "                st.subheader(\"Recommendation\")\n",
        "                st.write(\"\"\"\n",
        "                - **K-Fold:** Suitable for general cases with balanced datasets.\n",
        "                - **Stratified K-Fold:** Best for imbalanced classification problems.\n",
        "                - **Holdout:** Quick but less reliable for small datasets.\n",
        "                - **Leave-One-Out:** Best for very small datasets but computationally expensive.\n",
        "                - **Leave-P-Out:** Rarely used due to computational cost.\n",
        "                \"\"\")\n",
        "\n",
        "                # Display Cross-Validation Methods for Regressor and Classifier in a Table\n",
        "                st.subheader(\"Cross-Validation Best Practices\")\n",
        "                cv_table = pd.DataFrame({\n",
        "                    \"Method\": [\n",
        "                        \"K-Fold\",\n",
        "                        \"Stratified K-Fold\",\n",
        "                        \"Holdout (80/20 Split)\",\n",
        "                        \"Leave-One-Out\",\n",
        "                        \"Leave-P-Out\"\n",
        "                    ],\n",
        "                    \"Best for Classifier\": [\n",
        "                        \"Yes\",\n",
        "                        \"Yes\",\n",
        "                        \"No\",\n",
        "                        \"Yes\",\n",
        "                        \"No\"\n",
        "                    ],\n",
        "                    \"Best for Regressor\": [\n",
        "                        \"Yes\",\n",
        "                        \"No\",\n",
        "                        \"Yes\",\n",
        "                        \"Yes\",\n",
        "                        \"No\"\n",
        "                    ]\n",
        "                })\n",
        "                st.write(cv_table)\n",
        "\n",
        "with ml_tabs[3]:\n",
        "    st.subheader(\"Download Train and Test Datasets with Results\")\n",
        "\n",
        "    # Ensure features and target column are selected\n",
        "    if features and target_col:\n",
        "        if manual_task_type == \"Classification\":\n",
        "            # Reindex X_train and X_test to match the order of features used during training\n",
        "            X_train = X_train[features]\n",
        "            X_test = X_test[features]\n",
        "\n",
        "            # Generate results for the train dataset\n",
        "            train_result_labels = []\n",
        "            for true, pred in zip(y_train, model.predict(X_train)):\n",
        "                if true == 1 and pred == 1:\n",
        "                    train_result_labels.append(\"True Positive\")\n",
        "                elif true == 0 and pred == 0:\n",
        "                    train_result_labels.append(\"True Negative\")\n",
        "                elif true == 0 and pred == 1:\n",
        "                    train_result_labels.append(\"False Positive\")\n",
        "                elif true == 1 and pred == 0:\n",
        "                    train_result_labels.append(\"False Negative\")\n",
        "\n",
        "            train_data_with_results = X_train.copy()\n",
        "            train_data_with_results[target_col] = y_train.values\n",
        "            train_data_with_results[\"Predicted\"] = model.predict(X_train)\n",
        "            train_data_with_results[\"Result\"] = train_result_labels\n",
        "\n",
        "            # Generate results for the test dataset\n",
        "            test_result_labels = []\n",
        "            for true, pred in zip(y_test, y_pred):\n",
        "                if true == 1 and pred == 1:\n",
        "                    test_result_labels.append(\"True Positive\")\n",
        "                elif true == 0 and pred == 0:\n",
        "                    test_result_labels.append(\"True Negative\")\n",
        "                elif true == 0 and pred == 1:\n",
        "                    test_result_labels.append(\"False Positive\")\n",
        "                elif true == 1 and pred == 0:\n",
        "                    test_result_labels.append(\"False Negative\")\n",
        "\n",
        "            test_data_with_results = X_test.copy()\n",
        "            test_data_with_results[target_col] = y_test.values\n",
        "            test_data_with_results[\"Predicted\"] = y_pred\n",
        "            test_data_with_results[\"Result\"] = test_result_labels\n",
        "\n",
        "            # Provide download links for train and test datasets\n",
        "            st.subheader(\"Download Train Dataset with Results\")\n",
        "            train_buffer = io.BytesIO()\n",
        "            train_data_with_results.to_csv(train_buffer, index=False)\n",
        "            train_buffer.seek(0)\n",
        "            st.download_button(\n",
        "                label=\"Download Train Dataset with Results (CSV)\",\n",
        "                data=train_buffer,\n",
        "                file_name=\"train_dataset_with_results.csv\",\n",
        "                mime=\"text/csv\",\n",
        "            )\n",
        "\n",
        "            st.subheader(\"Download Test Dataset with Results\")\n",
        "            test_buffer = io.BytesIO()\n",
        "            test_data_with_results.to_csv(test_buffer, index=False)\n",
        "            test_buffer.seek(0)\n",
        "            st.download_button(\n",
        "                label=\"Download Test Dataset with Results (CSV)\",\n",
        "                data=test_buffer,\n",
        "                file_name=\"test_dataset_with_results.csv\",\n",
        "                mime=\"text/csv\",\n",
        "            )\n",
        "\n",
        "        elif manual_task_type == \"Regression\":\n",
        "            st.warning(\"This feature is only available for Classification tasks.\")\n",
        "    else:\n",
        "        st.warning(\"Please select features and a target column in the previous tab to proceed.\")\n",
        "\n",
        "with ml_tabs[4]:\n",
        "    st.subheader(\"Future Prediction\")\n",
        "\n",
        "    # Ensure model is trained and features are available\n",
        "    if 'model' in locals() and model is not None:\n",
        "        st.write(\"**Please input values for the following features to make a prediction:**\")\n",
        "\n",
        "        # Get feature names (assuming the model has been trained with 'X')\n",
        "        model_features = X.columns\n",
        "\n",
        "        # Create a dictionary to hold the user input\n",
        "        user_input = {}\n",
        "\n",
        "        # Create input fields for each feature\n",
        "        for feature in model_features:\n",
        "            user_input[feature] = st.text_input(f\"Enter value for {feature}:\")\n",
        "\n",
        "        # When the user presses the button to make a prediction\n",
        "        if st.button(\"Make Prediction\"):\n",
        "            try:\n",
        "                # Check if all the inputs are filled and valid\n",
        "                input_values = []\n",
        "                for feature in model_features:\n",
        "                    value = user_input[feature]\n",
        "                    # Convert to float and append to input_values\n",
        "                    input_values.append(float(value))  # Could raise ValueError if the input is not a valid number\n",
        "\n",
        "                # Ensure the input is in the correct shape (1, n_features)\n",
        "                input_values = np.array(input_values).reshape(1, -1)\n",
        "\n",
        "                # Make the prediction using the trained model\n",
        "                prediction = model.predict(input_values)\n",
        "\n",
        "                # Show the prediction result\n",
        "                st.write(f\"**Prediction Result:** {prediction[0]}\")\n",
        "\n",
        "            except ValueError:\n",
        "                st.error(\"Please enter valid numeric values for all features.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"An error occurred: {str(e)}\")\n",
        "    else:\n",
        "        st.warning(\"Model is not trained yet. Please train the model first.\")\n",
        "\n",
        "with ml_tabs[5]:\n",
        "    st.subheader(\"Unsupervised Learning Recommendations\")\n",
        "\n",
        "    # Option for selecting the clustering type\n",
        "    clustering_method = st.selectbox(\"Choose Clustering Method\", options=[\"KMeans\", \"Agglomerative Clustering\", \"DBSCAN\"])\n",
        "\n",
        "    if clustering_method == \"KMeans\":\n",
        "        st.subheader(\"KMeans Clustering\")\n",
        "\n",
        "        # Input for selecting features\n",
        "        features = st.multiselect(\"Select feature columns for clustering\", options=df.columns, default=df.select_dtypes(include=[np.number]).columns.tolist())\n",
        "\n",
        "        if features:\n",
        "            st.write(\"### Elbow Method: Find Optimal K for KMeans\")\n",
        "\n",
        "            # Elbow method for determining the optimal k\n",
        "            distortions = []\n",
        "            for k in range(1, 11):\n",
        "                kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "                kmeans.fit(df[features])\n",
        "                distortions.append(kmeans.inertia_)\n",
        "\n",
        "            # Plot the elbow method\n",
        "            fig, ax = plt.subplots()\n",
        "            ax.plot(range(1, 11), distortions, marker='o')\n",
        "            ax.set_xlabel('Number of Clusters (k)')\n",
        "            ax.set_ylabel('Distortion (Inertia)')\n",
        "            ax.set_title('Elbow Method for Optimal k')\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            optimal_k = st.number_input(\"Enter the optimal k value based on the Elbow method\", min_value=1, max_value=10, value=3)\n",
        "\n",
        "            # Apply KMeans with the optimal k\n",
        "            kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "            df['KMeans_Labels'] = kmeans.fit_predict(df[features])\n",
        "\n",
        "            # Silhouette Score\n",
        "            silhouette_avg = silhouette_score(df[features], df['KMeans_Labels'])\n",
        "            st.write(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
        "\n",
        "            # Number of clusters and their values\n",
        "            st.write(\"### KMeans Clusters\")\n",
        "            unique_clusters = np.unique(df['KMeans_Labels'])\n",
        "            for cluster in unique_clusters:\n",
        "                st.write(f\"Cluster {cluster}: {np.sum(df['KMeans_Labels'] == cluster)} points\")\n",
        "\n",
        "            # Visualize clusters in 2D\n",
        "            fig, ax = plt.subplots()\n",
        "            scatter = ax.scatter(df[features[0]], df[features[1]], c=df['KMeans_Labels'], cmap='viridis', alpha=0.6)\n",
        "            ax.set_xlabel(features[0])\n",
        "            ax.set_ylabel(features[1])\n",
        "            ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
        "            st.pyplot(fig)\n",
        "\n",
        "    elif clustering_method == \"Agglomerative Clustering\":\n",
        "        st.subheader(\"Agglomerative (Hierarchical) Clustering\")\n",
        "\n",
        "        # Input for selecting features\n",
        "        features = st.multiselect(\"Select feature columns for clustering\", options=df.columns, default=df.select_dtypes(include=[np.number]).columns.tolist())\n",
        "\n",
        "        if features:\n",
        "            st.write(\"### Dendrogram Method: Find Optimal Number of Clusters\")\n",
        "\n",
        "            # Hierarchical clustering linkage matrix\n",
        "            Z = linkage(df[features], method='ward')\n",
        "\n",
        "            # Plot the dendrogram\n",
        "            fig, ax = plt.subplots()\n",
        "            dendrogram(Z)\n",
        "            ax.set_title('Dendrogram for Agglomerative Clustering')\n",
        "            ax.set_xlabel('Sample index')\n",
        "            ax.set_ylabel('Distance')\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            # Input for cutting the dendrogram to determine clusters\n",
        "            max_d = st.number_input(\"Enter the maximum distance (cutoff) for clusters\", min_value=1, max_value=100000, value=5)\n",
        "\n",
        "            # Apply Agglomerative Clustering with the chosen cutoff\n",
        "            labels = fcluster(Z, max_d, criterion='distance')\n",
        "            df['Agglomerative_Labels'] = labels\n",
        "\n",
        "            # Display number of clusters\n",
        "            unique_clusters = np.unique(df['Agglomerative_Labels'])\n",
        "            st.write(\"### Agglomerative Clusters\")\n",
        "            for cluster in unique_clusters:\n",
        "                st.write(f\"Cluster {cluster}: {np.sum(df['Agglomerative_Labels'] == cluster)} points\")\n",
        "\n",
        "            # Compute Silhouette Score only if valid\n",
        "            if len(unique_clusters) > 1:\n",
        "                silhouette_avg = silhouette_score(df[features], df['Agglomerative_Labels'])\n",
        "                st.write(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
        "            else:\n",
        "                st.warning(\"Silhouette score requires at least 2 clusters. Please adjust the cutoff distance.\")\n",
        "\n",
        "            # Number of clusters and their values\n",
        "            st.write(\"### Agglomerative Clusters\")\n",
        "            unique_clusters = np.unique(df['Agglomerative_Labels'])\n",
        "            for cluster in unique_clusters:\n",
        "                st.write(f\"Cluster {cluster}: {np.sum(df['Agglomerative_Labels'] == cluster)} points\")\n",
        "\n",
        "            # Visualize clusters in 2D\n",
        "            fig, ax = plt.subplots()\n",
        "            scatter = ax.scatter(df[features[0]], df[features[1]], c=df['Agglomerative_Labels'], cmap='viridis', alpha=0.6)\n",
        "            ax.set_xlabel(features[0])\n",
        "            ax.set_ylabel(features[1])\n",
        "            ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
        "            st.pyplot(fig)\n",
        "\n",
        "    elif clustering_method == \"DBSCAN\":\n",
        "        st.subheader(\"DBSCAN Clustering\")\n",
        "\n",
        "        # Input for selecting features\n",
        "        features = st.multiselect(\"Select feature columns for clustering\", options=df.columns, default=df.select_dtypes(include=[np.number]).columns.tolist())\n",
        "\n",
        "        if features:\n",
        "            # Inputs for DBSCAN parameters\n",
        "            eps = st.number_input(\"Enter radius (eps)\", min_value=0.1, max_value=10.0, value=0.5)\n",
        "            min_samples = st.number_input(\"Enter minimum number of samples (min_samples)\", min_value=2, max_value=10, value=5)\n",
        "\n",
        "            # Apply DBSCAN\n",
        "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "            df['DBSCAN_Labels'] = dbscan.fit_predict(df[features])\n",
        "\n",
        "            # Silhouette Score\n",
        "            if len(set(df['DBSCAN_Labels'])) > 1:  # Silhouette score requires more than 1 cluster\n",
        "                silhouette_avg = silhouette_score(df[features], df['DBSCAN_Labels'])\n",
        "                st.write(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
        "            else:\n",
        "                st.write(\"DBSCAN resulted in a single cluster or noise. Silhouette score is not applicable.\")\n",
        "\n",
        "            # Number of clusters and their values\n",
        "            st.write(\"### DBSCAN Clusters\")\n",
        "            unique_clusters = np.unique(df['DBSCAN_Labels'])\n",
        "            for cluster in unique_clusters:\n",
        "                if cluster == -1:\n",
        "                    st.write(f\"Noise/Outliers (Cluster {cluster}): {np.sum(df['DBSCAN_Labels'] == cluster)} points\")\n",
        "                else:\n",
        "                    st.write(f\"Cluster {cluster}: {np.sum(df['DBSCAN_Labels'] == cluster)} points\")\n",
        "\n",
        "            # Visualize clusters in 3D if more than 2 features are selected\n",
        "            if len(features) >= 3:\n",
        "                fig = plt.figure()\n",
        "                ax = fig.add_subplot(111, projection='3d')\n",
        "                scatter = ax.scatter(df[features[0]], df[features[1]], df[features[2]], c=df['DBSCAN_Labels'], cmap='viridis', alpha=0.6)\n",
        "                ax.set_xlabel(features[0])\n",
        "                ax.set_ylabel(features[1])\n",
        "                ax.set_zlabel(features[2])\n",
        "                ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
        "                st.pyplot(fig)\n",
        "            else:\n",
        "                # If less than 3 features, plot 2D visualization\n",
        "                fig, ax = plt.subplots()\n",
        "                scatter = ax.scatter(df[features[0]], df[features[1]], c=df['DBSCAN_Labels'], cmap='viridis', alpha=0.6)\n",
        "                ax.set_xlabel(features[0])\n",
        "                ax.set_ylabel(features[1])\n",
        "                ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
        "                st.pyplot(fig)\n",
        "\n",
        "    # Allow the user to download the clustered dataset\n",
        "    st.subheader(\"Download Clustered Dataset\")\n",
        "    csv = df.to_csv(index=False)\n",
        "    st.download_button(label=\"Download CSV\", data=csv, file_name=\"clustered_dataset.csv\", mime=\"text/csv\")\n",
        "\n",
        "with ml_tabs[6]:\n",
        "    st.subheader(\"Hyperparameter Tuning\")\n",
        "\n",
        "    st.write(\"### What is Hyperparameter Tuning?\")\n",
        "    st.write(\n",
        "        \"Hyperparameter tuning is the process of finding the optimal set of hyperparameters for a machine learning model. \"\n",
        "        \"Hyperparameters are different from model parameters in that they cannot be learned directly from the training process. \"\n",
        "        \"Instead, they are set manually and control various aspects of the learning process, such as model complexity, learning rate, and decision thresholds. \"\n",
        "        \"Tuning these hyperparameters can significantly improve model performance.\"\n",
        "    )\n",
        "\n",
        "    st.write(\"### Why is Hyperparameter Tuning Important?\")\n",
        "    st.write(\n",
        "        \"1. **Model Performance:** Helps achieve higher accuracy, better generalization, and reduced overfitting.\\n\"\n",
        "        \"2. **Efficiency:** Efficient models train faster when parameters are well-tuned.\\n\"\n",
        "        \"3. **Robustness:** Leads to more stable and reliable models under different data conditions.\"\n",
        "    )\n",
        "\n",
        "    st.write(\"### Common Hyperparameter Tuning Algorithms\")\n",
        "\n",
        "    st.write(\"#### 1. Grid Search\")\n",
        "    st.write(\n",
        "        \"Grid search involves specifying a set of hyperparameters and trying all possible combinations systematically. \"\n",
        "        \"It evaluates model performance for each combination and selects the best one based on a performance metric such as accuracy or F1 score.\"\n",
        "    )\n",
        "\n",
        "    st.code(\n",
        "        \"\"\"\n",
        "        from sklearn.model_selection import GridSearchCV\n",
        "        from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "        param_grid = {\n",
        "            'n_estimators': [50, 100, 150],\n",
        "            'max_depth': [10, 20, 30]\n",
        "        }\n",
        "\n",
        "        model = RandomForestClassifier()\n",
        "        grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        print(\"Best parameters found: \", grid_search.best_params_)\n",
        "        \"\"\",\n",
        "        language='python'\n",
        "    )\n",
        "\n",
        "    st.write(\"#### 2. Random Search\")\n",
        "    st.write(\n",
        "        \"Unlike grid search, random search randomly selects hyperparameter combinations to evaluate rather than systematically exploring all options. \"\n",
        "        \"It is faster and often achieves similar results when given enough computation time.\"\n",
        "    )\n",
        "\n",
        "    st.code(\n",
        "        \"\"\"\n",
        "        from sklearn.model_selection import RandomizedSearchCV\n",
        "        from sklearn.ensemble import RandomForestClassifier\n",
        "        from scipy.stats import randint\n",
        "\n",
        "        param_dist = {\n",
        "            'n_estimators': randint(50, 200),\n",
        "            'max_depth': randint(5, 40)\n",
        "        }\n",
        "\n",
        "        model = RandomForestClassifier()\n",
        "        random_search = RandomizedSearchCV(model, param_dist, cv=5, n_iter=10)\n",
        "        random_search.fit(X_train, y_train)\n",
        "        print(\"Best parameters found: \", random_search.best_params_)\n",
        "        \"\"\",\n",
        "        language='python'\n",
        "    )\n",
        "\n",
        "    st.write(\"### Conclusion\")\n",
        "    st.write(\n",
        "        \"Hyperparameter tuning is essential for optimizing machine learning models. Tools like GridSearchCV and RandomizedSearchCV \"\n",
        "        \"make it easier to experiment and identify the most effective parameter configurations, leading to improved model performance.\"\n",
        "    )\n",
        "\n",
        "# Final Notes\n",
        "st.sidebar.info(\n",
        "    \"\"\"\n",
        "    **Instructions:**\n",
        "    - Upload your datasets in CSV, Excel, or JSON format.\n",
        "    - Navigate through the tabs to explore dataset insights, supervised/unsupervised learning recommendations, and data quality insights.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "st.sidebar.success(\"Developed by [ TC.Antony - Data Scientist] \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrdxY1znWvHW",
        "outputId": "45485d63-bc73-488b-de2a-d064dcaf3eac"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace 'YOUR_AUTHTOKEN' with your actual ngrok authtoken\n",
        "ngrok.set_auth_token(\"2rI2XurhgC2fxlYDtteHntWpCJf_5b1kDx2SLmwgq8GukDEyc\")\n",
        "\n",
        "# Run the Streamlit app in the background\n",
        "!streamlit run app1.py &>/dev/null&\n",
        "\n",
        "# Create a public URL using ngrok\n",
        "try:\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(f\"Streamlit app is running at {public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Trying to run with localtunnel\")\n",
        "    !streamlit run app1.py &>/content/logs.txt & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6a-REFMxCPh",
        "outputId": "f5c083a1-2068-44ab-d818-8298087a3755"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is running at NgrokTunnel: \"https://8d06-34-125-143-13.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ipv4 = !curl ipv4.icanhazip.com\n",
        "ipv4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNa_hmhBLjDw",
        "outputId": "f2557e64-4337-473d-e698-765255fb6279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['34.90.213.111']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sxiqP5cEbWng"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}